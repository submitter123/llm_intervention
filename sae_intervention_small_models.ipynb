{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb05d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from utils.utils_exp import run_sae_intervention_on_small_models\n",
    "from utils.utils_model import get_pythia_70m, get_gpt2_small\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = \"results/pythia_gpt2\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "print(f\"Results will be saved to: {results_dir}\")\n",
    "\n",
    "# Helper function to save results\n",
    "def save_experiment_results(results, model_name, suffix=\"\"):\n",
    "    \"\"\"Save experiment results to JSON file\"\"\"\n",
    "    filename = f\"{model_name}_sae_intervention_results{suffix}.json\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "    \n",
    "    # Convert tensors to serializable format\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().numpy().tolist()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    serializable_results = convert_to_serializable(results)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results saved to: {filepath}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be4aadf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration for Pythia\n",
    "pythia_model, pythia_tokenizer = get_pythia_70m(DEVICE)\n",
    "PYTHIA_CONFIG = {\n",
    "    'model_name': 'pythia',\n",
    "    'layer_types': ['att', 'mlp', 'res'],\n",
    "    'layer_range': range(0, 6),\n",
    "    'model': pythia_model,\n",
    "    'tokenizer': pythia_tokenizer,\n",
    "    'similarity_threshold': 0.2,\n",
    "    'n_target_features': 10,\n",
    "    'n_interference_features': 3,\n",
    "    'n_top_tokens': 5,\n",
    "    'n_test_sentences': 3,\n",
    "    'seed': 50,\n",
    "    'device': DEVICE,\n",
    "    'use_auto_model': True\n",
    "}\n",
    "\n",
    "# Scale values for intervention\n",
    "scale_values = [0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 17, 20]\n",
    "SCALE_RANGE = [-x for x in scale_values[::-1]] + scale_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "pythia_results = run_sae_intervention_on_small_models(\n",
    "    scale_range=SCALE_RANGE,\n",
    "    **PYTHIA_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2dd63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Pythia results\n",
    "import time\n",
    "timestamp = str(int(time.time()))\n",
    "thres = PYTHIA_CONFIG['similarity_threshold']\n",
    "pythia_filepath = save_experiment_results(pythia_results, 'pythia', f'_{thres}_{timestamp}')\n",
    "\n",
    "# Print detailed statistics\n",
    "total_features = 0\n",
    "total_tests = 0\n",
    "layer_stats = {}\n",
    "\n",
    "for layer_type in pythia_results:\n",
    "    layer_stats[layer_type] = {}\n",
    "    for layer_idx in pythia_results[layer_type]:\n",
    "        features_in_layer = len(pythia_results[layer_type][layer_idx])\n",
    "        layer_stats[layer_type][layer_idx] = features_in_layer\n",
    "        total_features += features_in_layer\n",
    "        \n",
    "        # Count total tests (tokens x sentences)\n",
    "        for feature_id in pythia_results[layer_type][layer_idx]:\n",
    "            feature_data = pythia_results[layer_type][layer_idx][feature_id]\n",
    "            total_tests += len(feature_data.get('tests', {}))\n",
    "\n",
    "print(f\"\\nPythia Results Summary:\")\n",
    "print(f\"- Total features tested: {total_features}\")\n",
    "print(f\"- Total token tests: {total_tests}\")\n",
    "print(f\"- Layer types: {list(pythia_results.keys())}\")\n",
    "\n",
    "for layer_type, layers in layer_stats.items():\n",
    "    print(f\"\\n{layer_type.upper()} layers:\")\n",
    "    for layer_idx, feature_count in layers.items():\n",
    "        print(f\"  Layer {layer_idx}: {feature_count} features\")\n",
    "        \n",
    "print(f\"\\nFull results saved to: {pythia_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ce20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "gpt2_model, gpt2_tokenizer = get_gpt2_small(DEVICE)\n",
    "\n",
    "GPT2_CONFIG = {\n",
    "    'model_name': 'gpt2',\n",
    "    'layer_types': ['att', 'res_mid', 'mlp', 'res_post'],\n",
    "    'layer_range': range(0, 12),  # Layers 0-11\n",
    "    'model': gpt2_model,\n",
    "    'tokenizer': gpt2_tokenizer,\n",
    "    'n_target_features': 10,\n",
    "    'n_interference_features': 3,\n",
    "    'n_top_tokens': 5,\n",
    "    'n_test_sentences': 3,\n",
    "    'seed': 53,\n",
    "    'device': DEVICE,\n",
    "    'use_auto_model': True\n",
    "}\n",
    "\n",
    "semantic_thresholds = [0.4, 0.3, 0.2, 0.15]\n",
    "\n",
    "scale_values = [0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 17, 20]\n",
    "SCALE_RANGE = [-x for x in scale_values[::-1]] + scale_values\n",
    "\n",
    "print(f\"Starting GPT-2 SAE experiments with semantic thresholds: {semantic_thresholds}\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Layer types: {GPT2_CONFIG['layer_types']}\")\n",
    "print(f\"- Layer range: {list(GPT2_CONFIG['layer_range'])}\")\n",
    "print(f\"- Target features: {GPT2_CONFIG['n_target_features']}\")\n",
    "print(f\"- Interference features: {GPT2_CONFIG['n_interference_features']}\")\n",
    "print(f\"- Top tokens: {GPT2_CONFIG['n_top_tokens']}\")\n",
    "print(f\"- Test sentences: {GPT2_CONFIG['n_test_sentences']}\")\n",
    "print(f\"- Scale range: {len(SCALE_RANGE)} values from {min(SCALE_RANGE)} to {max(SCALE_RANGE)}\")\n",
    "\n",
    "gpt2_results = {}\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i, threshold in enumerate(semantic_thresholds):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running GPT-2 SAE experiment {i+1}/{len(semantic_thresholds)} with similarity threshold: {threshold}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    config = GPT2_CONFIG.copy()\n",
    "    config['similarity_threshold'] = threshold\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = run_sae_intervention_on_small_models(\n",
    "            scale_range=SCALE_RANGE,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        gpt2_results[threshold] = results\n",
    "        \n",
    "        total_features = 0\n",
    "        total_tests = 0\n",
    "        layer_stats = {}\n",
    "        \n",
    "        for layer_type in results:\n",
    "            layer_stats[layer_type] = {}\n",
    "            for layer_idx in results[layer_type]:\n",
    "                features_in_layer = len(results[layer_type][layer_idx])\n",
    "                layer_stats[layer_type][layer_idx] = features_in_layer\n",
    "                total_features += features_in_layer\n",
    "                \n",
    "                for feature_id in results[layer_type][layer_idx]:\n",
    "                    feature_data = results[layer_type][layer_idx][feature_id]\n",
    "                    total_tests += len(feature_data.get('tests', {}))\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        \n",
    "        print(f\"\\nCompleted threshold {threshold} in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Total elapsed time: {total_elapsed:.2f} seconds\")\n",
    "        print(f\"Statistics for threshold {threshold}:\")\n",
    "        print(f\"- Total features tested: {total_features}\")\n",
    "        print(f\"- Total token tests: {total_tests}\")\n",
    "        print(f\"- Layer types: {list(results.keys())}\")\n",
    "        \n",
    "        for layer_type, layers in layer_stats.items():\n",
    "            layer_feature_count = sum(layers.values())\n",
    "            print(f\"  {layer_type.upper()}: {layer_feature_count} features across {len(layers)} layers\")\n",
    "        \n",
    "        timestamp = str(int(time.time()))\n",
    "        filepath = save_experiment_results(\n",
    "            gpt2_results[threshold], \n",
    "            'gpt2', \n",
    "            f'_{threshold}_{timestamp}'\n",
    "        )\n",
    "        \n",
    "        print(f\"- Results saved to: {filepath}\")\n",
    "        \n",
    "        remaining_thresholds = len(semantic_thresholds) - (i + 1)\n",
    "        if remaining_thresholds > 0:\n",
    "            avg_time_per_threshold = total_elapsed / (i + 1)\n",
    "            estimated_remaining = avg_time_per_threshold * remaining_thresholds\n",
    "            print(f\"- Estimated remaining time: {estimated_remaining:.1f} seconds ({estimated_remaining/60:.1f} minutes)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running experiment with threshold {threshold}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        gpt2_results[threshold] = {'error': str(e)}\n",
    "        continue\n",
    "\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All GPT-2 SAE experiments completed!\")\n",
    "print(f\"Total execution time: {total_elapsed_time:.2f} seconds ({total_elapsed_time/60:.1f} minutes)\")\n",
    "print(f\"Successfully completed thresholds: {[t for t in gpt2_results.keys() if 'error' not in gpt2_results[t]]}\")\n",
    "print(f\"Failed thresholds: {[t for t in gpt2_results.keys() if 'error' in gpt2_results[t]]}\")\n",
    "\n",
    "print(f\"\\nOverall GPT-2 SAE Results Summary:\")\n",
    "for threshold in semantic_thresholds:\n",
    "    if threshold in gpt2_results and 'error' not in gpt2_results[threshold]:\n",
    "        results = gpt2_results[threshold]\n",
    "        total_features = sum(len(results[lt][li]) for lt in results for li in results[lt])\n",
    "        total_tests = sum(len(results[lt][li][fid].get('tests', {})) \n",
    "                         for lt in results for li in results[lt] for fid in results[lt][li])\n",
    "        \n",
    "        print(f\"Threshold {threshold}:\")\n",
    "        print(f\"  - Features: {total_features}\")\n",
    "        print(f\"  - Tests: {total_tests}\")\n",
    "        print(f\"  - Layer types: {list(results.keys())}\")\n",
    "    else:\n",
    "        print(f\"Threshold {threshold}: FAILED\")\n",
    "\n",
    "print(f\"\\nAll result files are saved in the '{results_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed23069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Load GPT-2 model\n",
    "gpt2_model, gpt2_tokenizer = get_gpt2_small(DEVICE)\n",
    "\n",
    "GPT2_CONFIG = {\n",
    "    'model_name': 'gpt2',\n",
    "    'layer_types': ['att', 'res_mid', 'mlp', 'res_post'],\n",
    "    'layer_range': range(0, 12),  # Layers 0-11\n",
    "    'model': gpt2_model,  # Use AutoModel\n",
    "    'tokenizer': gpt2_tokenizer,  # Add tokenizer\n",
    "    'n_target_features': 3,\n",
    "    'n_interference_features': 3,\n",
    "    'n_top_tokens': 3,\n",
    "    'n_test_sentences': 3,\n",
    "    'seed': 42,\n",
    "    'device': DEVICE,\n",
    "    'use_auto_model': True  # Add this flag\n",
    "}\n",
    "\n",
    "semantic_thresholds = [0.4, 0.3, 0.2, 0.15]\n",
    "\n",
    "scale_values = [0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 17, 20]\n",
    "SCALE_RANGE = [-x for x in scale_values[::-1]] + scale_values\n",
    "\n",
    "print(f\"Starting GPT-2 experiments with semantic thresholds: {semantic_thresholds}\")\n",
    "print(f\"Layer types: {GPT2_CONFIG['layer_types']}\")\n",
    "print(f\"Layer range: {list(GPT2_CONFIG['layer_range'])}\")\n",
    "print(f\"Scale range: {len(SCALE_RANGE)} values from {min(SCALE_RANGE)} to {max(SCALE_RANGE)}\")\n",
    "\n",
    "gpt2_results = {}\n",
    "\n",
    "for threshold in semantic_thresholds:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running GPT-2 experiment with similarity threshold: {threshold}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config = GPT2_CONFIG.copy()\n",
    "    config['similarity_threshold'] = threshold\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = run_sae_intervention_on_small_models(\n",
    "            scale_range=SCALE_RANGE,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        gpt2_results[threshold] = results\n",
    "        \n",
    "        total_features = 0\n",
    "        total_tests = 0\n",
    "        layer_stats = {}\n",
    "        \n",
    "        for layer_type in results:\n",
    "            layer_stats[layer_type] = {}\n",
    "            for layer_idx in results[layer_type]:\n",
    "                features_in_layer = len(results[layer_type][layer_idx])\n",
    "                layer_stats[layer_type][layer_idx] = features_in_layer\n",
    "                total_features += features_in_layer\n",
    "                \n",
    "                for feature_id in results[layer_type][layer_idx]:\n",
    "                    feature_data = results[layer_type][layer_idx][feature_id]\n",
    "                    total_tests += len(feature_data.get('tests', {}))\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nCompleted threshold {threshold} in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"- Total features tested: {total_features}\")\n",
    "        print(f\"- Total token tests: {total_tests}\")\n",
    "        \n",
    "        timestamp = str(int(time.time()))\n",
    "        filename = f\"gpt2_sae_intervention_results_{threshold}_{timestamp}.json\"\n",
    "        filepath = save_experiment_results(gpt2_results[threshold], 'gpt2', f'_{threshold}_{timestamp}')\n",
    "        \n",
    "        print(f\"- Results saved to: {filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running experiment with threshold {threshold}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All GPT-2 experiments completed!\")\n",
    "print(f\"Successfully completed thresholds: {list(gpt2_results.keys())}\")\n",
    "\n",
    "print(f\"\\nOverall GPT-2 Results Summary:\")\n",
    "for threshold in gpt2_results:\n",
    "    results = gpt2_results[threshold]\n",
    "    total_features = sum(len(results[lt][li]) for lt in results for li in results[lt])\n",
    "    total_tests = sum(len(results[lt][li][fid].get('tests', {})) \n",
    "                     for lt in results for li in results[lt] for fid in results[lt][li])\n",
    "    \n",
    "    print(f\"Threshold {threshold}:\")\n",
    "    print(f\"  - Features: {total_features}\")\n",
    "    print(f\"  - Tests: {total_tests}\")\n",
    "    print(f\"  - Layer types: {list(results.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d120717",
   "metadata": {},
   "source": [
    "## plot absolute improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "def plot_sae_intervention_results(data_path, output_dir=\"analysis_results\", model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    A temporary plot function\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if '*' in data_path:\n",
    "        files = glob.glob(data_path)\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No files found matching pattern: {data_path}\")\n",
    "        data_path = sorted(files)[-1]\n",
    "        print(f\"Using latest file: {data_path}\")\n",
    "    \n",
    "    with open(data_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded results from: {data_path}\")\n",
    "    \n",
    "    metrics_data = {\n",
    "        'weighted_cosine_similarity': defaultdict(list),\n",
    "        'spearman_correlation': defaultdict(list),\n",
    "        'kendall_correlation': defaultdict(list),\n",
    "        'weighted_overlap': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    metric_names = {\n",
    "        'weighted_cosine_similarity': 'Weighted Cosine Similarity',\n",
    "        'spearman_correlation': 'Spearman Correlation',\n",
    "        'kendall_correlation': 'Kendall Correlation',\n",
    "        'weighted_overlap': 'Weighted Overlap'\n",
    "    }\n",
    "    \n",
    "    print(\"Processing results...\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    for layer_type in results:\n",
    "        for layer_index in results[layer_type]:\n",
    "            for feature_id in results[layer_type][layer_index]:\n",
    "                feature_data = results[layer_type][layer_index][feature_id]\n",
    "                \n",
    "                for token_idx in feature_data.get('tests', {}):\n",
    "                    token_data = feature_data['tests'][token_idx]\n",
    "                    \n",
    "                    for sent_idx in token_data.get('sentences', {}):\n",
    "                        sent_data = token_data['sentences'][sent_idx]\n",
    "                        \n",
    "                        if 'baseline' not in sent_data:\n",
    "                            continue\n",
    "                        \n",
    "                        baseline = sent_data['baseline']\n",
    "                        \n",
    "                        for level in sent_data:\n",
    "                            if level == 'baseline':\n",
    "                                continue\n",
    "                            \n",
    "                            level_data = sent_data[level]\n",
    "                            \n",
    "                            for metric in metrics_data.keys():\n",
    "                                baseline_val = baseline.get(metric, 0)\n",
    "                                best_val = level_data.get(f'best_{metric}', 0)\n",
    "                                \n",
    "                                improvement = best_val - baseline_val\n",
    "                                \n",
    "                                if improvement > 0:\n",
    "                                    metrics_data[metric][level].append(improvement)\n",
    "                        \n",
    "                        total_processed += 1\n",
    "    \n",
    "    print(f\"Processed {total_processed} test cases\")\n",
    "    \n",
    "    all_levels = set()\n",
    "    for metric in metrics_data:\n",
    "        all_levels.update(metrics_data[metric].keys())\n",
    "    \n",
    "    level_order = ['self']\n",
    "    \n",
    "    range_levels = [l for l in all_levels if '-' in l and l != 'self']\n",
    "    range_levels.sort(key=lambda x: float(x.split('-')[0]) if x.split('-')[0].replace('.','').isdigit() else 0, reverse=True)\n",
    "    level_order.extend(range_levels)\n",
    "    \n",
    "    if 'rand' in all_levels:\n",
    "        level_order.append('rand')\n",
    "    \n",
    "    other_levels = [l for l in all_levels if l not in level_order]\n",
    "    level_order.extend(sorted(other_levels))\n",
    "    \n",
    "    print(f\"Interference levels found: {level_order}\")\n",
    "    \n",
    "    level_labels = {}\n",
    "    for level in level_order:\n",
    "        if level == 'self':\n",
    "            level_labels[level] = 'Self'\n",
    "        elif level == 'rand':\n",
    "            level_labels[level] = 'Random'\n",
    "        elif '-' in level and level.count('-') == 1:\n",
    "            try:\n",
    "                parts = level.split('-')\n",
    "                min_val = float(parts[0])\n",
    "                if min_val >= 0.4:\n",
    "                    level_labels[level] = f\"High\\n({level})\"\n",
    "                elif min_val >= 0.3:\n",
    "                    level_labels[level] = f\"H-Mid\\n({level})\"\n",
    "                elif min_val >= 0.2:\n",
    "                    level_labels[level] = f\"Mid\\n({level})\"\n",
    "                elif min_val >= 0.1:\n",
    "                    level_labels[level] = f\"L-Mid\\n({level})\"\n",
    "                else:\n",
    "                    level_labels[level] = f\"Low\\n({level})\"\n",
    "            except:\n",
    "                level_labels[level] = level.title()\n",
    "        else:\n",
    "            level_labels[level] = level.title()\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12), dpi=300)\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    colors = plt.cm.Blues(np.linspace(0.3, 0.9, len(level_order)))\n",
    "    \n",
    "    metrics = ['weighted_cosine_similarity', 'spearman_correlation', 'kendall_correlation', 'weighted_overlap']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        means = []\n",
    "        sems = []\n",
    "        labels = []\n",
    "        \n",
    "        for level in level_order:\n",
    "            if level in metrics_data[metric] and len(metrics_data[metric][level]) > 0:\n",
    "                values = metrics_data[metric][level]\n",
    "                means.append(np.mean(values))\n",
    "                sems.append(np.std(values) / np.sqrt(len(values)))\n",
    "                labels.append(level_labels[level])\n",
    "            else:\n",
    "                means.append(0)\n",
    "                sems.append(0)\n",
    "                labels.append(level_labels.get(level, level))\n",
    "        \n",
    "        x_positions = np.arange(len(means))\n",
    "        \n",
    "        bars = ax.bar(x_positions, means, width=0.6, color=colors, \n",
    "                      yerr=sems, capsize=3, \n",
    "                      error_kw={'ecolor': 'black', 'linewidth': 1, 'capthick': 1})\n",
    "        \n",
    "        for idx, (bar, mean_val, sem_val) in enumerate(zip(bars, means, sems)):\n",
    "            if mean_val > 0:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + sem_val + max(means)*0.01,\n",
    "                       f'{mean_val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "        \n",
    "        ax.set_title(metric_names[metric], fontsize=14, pad=15, fontweight='bold')\n",
    "        ax.set_xlabel('Interference Level', fontsize=12)\n",
    "        ax.set_ylabel('Mean Improvement', fontsize=12)\n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=10)\n",
    "        ax.grid(axis='y', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        ax.set_ylim(bottom=0)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        \n",
    "        total_points = sum(len(metrics_data[metric][level]) for level in level_order \n",
    "                          if level in metrics_data[metric])\n",
    "        ax.text(0.02, 0.98, f'n={total_points}', transform=ax.transAxes, \n",
    "                fontsize=9, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle(f'{model_name} SAE Intervention Results', fontsize=16, y=0.95, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    model_lower = model_name.lower().replace(' ', '_')\n",
    "    png_path = os.path.join(output_dir, f\"{model_lower}_sae_intervention_results.png\")\n",
    "    pdf_path = os.path.join(output_dir, f\"{model_lower}_sae_intervention_results.pdf\")\n",
    "    \n",
    "    plt.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(pdf_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    statistics = {}\n",
    "    for metric in metrics_data:\n",
    "        statistics[metric] = {}\n",
    "        metric_data = metrics_data[metric]\n",
    "        \n",
    "        for level in metric_data:\n",
    "            if len(metric_data[level]) > 0:\n",
    "                values = metric_data[level]\n",
    "                statistics[metric][level] = {\n",
    "                    'mean': float(np.mean(values)),\n",
    "                    'std': float(np.std(values)),\n",
    "                    'sem': float(np.std(values) / np.sqrt(len(values))),\n",
    "                    'count': len(values),\n",
    "                    'min': float(np.min(values)),\n",
    "                    'max': float(np.max(values)),\n",
    "                    'median': float(np.median(values))\n",
    "                }\n",
    "            else:\n",
    "                statistics[metric][level] = {\n",
    "                    'mean': 0, 'std': 0, 'sem': 0, 'count': 0,\n",
    "                    'min': 0, 'max': 0, 'median': 0\n",
    "                }\n",
    "    \n",
    "    stats_path = os.path.join(output_dir, f\"{model_lower}_statistics.json\")\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(statistics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved:\")\n",
    "    print(f\"- Plot (PNG): {png_path}\")\n",
    "    print(f\"- Plot (PDF): {pdf_path}\")\n",
    "    print(f\"- Statistics: {stats_path}\")\n",
    "    \n",
    "    return statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a7f5d",
   "metadata": {},
   "source": [
    "## plot relative improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2411e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "\n",
    "def plot_sae_intervention_results(data_path, output_dir=\"analysis_results\", model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    A temporary plot function for relative value improvements\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        font_path = \"./AvenirLTStd-Roman.otf\"\n",
    "        if os.path.exists(font_path):\n",
    "            avenir_font = fm.FontProperties(fname=font_path)\n",
    "            fm.fontManager.addfont(font_path)\n",
    "            mpl.rcParams['font.family'] = avenir_font.get_name()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if '*' in data_path:\n",
    "        files = glob.glob(data_path)\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No files found matching pattern: {data_path}\")\n",
    "        data_path = sorted(files)[-1]\n",
    "        print(f\"Using latest file: {data_path}\")\n",
    "    \n",
    "    with open(data_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded results from: {data_path}\")\n",
    "    \n",
    "    metrics_data = {\n",
    "        'weighted_cosine_similarity': defaultdict(list),\n",
    "        'spearman_correlation': defaultdict(list),\n",
    "        'kendall_correlation': defaultdict(list),\n",
    "        'weighted_overlap': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    metric_names = {\n",
    "        'weighted_cosine_similarity': 'Weighted Cosine Similarity',\n",
    "        'spearman_correlation': 'Spearman Correlation',\n",
    "        'kendall_correlation': 'Kendall Correlation',\n",
    "        'weighted_overlap': 'Weighted Overlap'\n",
    "    }\n",
    "    \n",
    "    print(\"Processing results...\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_improved = 0\n",
    "    improvement_counts = {metric: defaultdict(int) for metric in metrics_data.keys()}\n",
    "    \n",
    "    for layer_type in results:\n",
    "        for layer_index in results[layer_type]:\n",
    "            for feature_id in results[layer_type][layer_index]:\n",
    "                feature_data = results[layer_type][layer_index][feature_id]\n",
    "                \n",
    "                for token_idx in feature_data.get('tests', {}):\n",
    "                    token_data = feature_data['tests'][token_idx]\n",
    "                    \n",
    "                    for sent_idx in token_data.get('sentences', {}):\n",
    "                        sent_data = token_data['sentences'][sent_idx]\n",
    "                    \n",
    "                        if 'baseline' not in sent_data:\n",
    "                            continue\n",
    "                        \n",
    "                        baseline = sent_data['baseline']\n",
    "                        \n",
    "                        for level in sent_data:\n",
    "                            if level == 'baseline':\n",
    "                                continue\n",
    "                            \n",
    "                            level_data = sent_data[level]\n",
    "                            \n",
    "                            for metric in metrics_data.keys():\n",
    "                                baseline_val = baseline.get(metric, 0)\n",
    "                                best_val = level_data.get(f'best_{metric}', 0)\n",
    "                                \n",
    "                                is_improved = False\n",
    "                                percentage_improvement = 0\n",
    "                                \n",
    "                                if metric in ['weighted_cosine_similarity', 'weighted_overlap']:\n",
    "                                    is_improved = best_val > baseline_val\n",
    "                                    if is_improved and abs(baseline_val) > 1e-8:\n",
    "                                        percentage_improvement = (best_val - baseline_val) / abs(baseline_val) * 100\n",
    "                                        \n",
    "                                elif metric in ['spearman_correlation', 'kendall_correlation']:\n",
    "                                    is_improved = best_val > baseline_val\n",
    "                                    if is_improved and abs(baseline_val) > 1e-8:\n",
    "                                        percentage_improvement = (best_val - baseline_val) / abs(baseline_val) * 100\n",
    "                                \n",
    "                                if is_improved and percentage_improvement > 0:\n",
    "                                    metrics_data[metric][level].append(percentage_improvement)\n",
    "                                    improvement_counts[metric][level] += 1\n",
    "                                    total_improved += 1\n",
    "                        \n",
    "                        total_processed += 1\n",
    "    \n",
    "    print(f\"Processed {total_processed} test cases\")\n",
    "    print(f\"Total improvements recorded: {total_improved}\")\n",
    "\n",
    "    print(\"\\nImprovement counts per metric and level:\")\n",
    "    for metric in metrics_data.keys():\n",
    "        print(f\"\\n{metric_names[metric]}:\")\n",
    "        for level in sorted(improvement_counts[metric].keys()):\n",
    "            count = improvement_counts[metric][level]\n",
    "            print(f\"  {level:15s}: {count:3d} improvements\")\n",
    "    \n",
    "    all_levels = set()\n",
    "    for metric in metrics_data:\n",
    "        all_levels.update(metrics_data[metric].keys())\n",
    "    \n",
    "    level_order = ['self']\n",
    "    \n",
    "    range_levels = [l for l in all_levels if '-' in l and l != 'self']\n",
    "    range_levels.sort(key=lambda x: float(x.split('-')[0]) if x.split('-')[0].replace('.','').isdigit() else 0, reverse=True)\n",
    "    level_order.extend(range_levels)\n",
    "    \n",
    "    if 'rand' in all_levels:\n",
    "        level_order.append('rand')\n",
    "    \n",
    "    other_levels = [l for l in all_levels if l not in level_order]\n",
    "    level_order.extend(sorted(other_levels))\n",
    "    \n",
    "    print(f\"\\nInterference levels found: {level_order}\")\n",
    "    \n",
    "    level_labels = {}\n",
    "    for level in level_order:\n",
    "        if level == 'self':\n",
    "            level_labels[level] = 'Original'\n",
    "        elif level == 'rand':\n",
    "            level_labels[level] = 'Random'\n",
    "        elif '-' in level and level.count('-') == 1:\n",
    "            try:\n",
    "                parts = level.split('-')\n",
    "                min_val = float(parts[0])\n",
    "                if min_val >= 0.4:\n",
    "                    level_labels[level] = \"High\"\n",
    "                elif min_val >= 0.3:\n",
    "                    level_labels[level] = \"Mid High\"\n",
    "                elif min_val >= 0.2:\n",
    "                    level_labels[level] = \"Mid\"\n",
    "                elif min_val >= 0.1:\n",
    "                    level_labels[level] = \"Mid Low\"\n",
    "                else:\n",
    "                    level_labels[level] = \"Low\"\n",
    "            except:\n",
    "                level_labels[level] = level.title()\n",
    "        else:\n",
    "            level_labels[level] = level.title()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(17, 4.5), dpi=300)\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    colors = ['#0D47A1', '#1565C0', '#1976D2', '#1E88E5', '#2196F3', '#42A5F5', '#64B5F6']\n",
    "    \n",
    "    metrics = ['weighted_cosine_similarity', 'spearman_correlation', 'kendall_correlation', 'weighted_overlap']\n",
    "    titles = ['Weighted Cosine Similarity', 'Spearman Correlation', 'Kendall Correlation', 'Weighted Overlap']\n",
    "    y_labels = [\n",
    "        'Mean % Improvement',\n",
    "        'Mean % Improvement', \n",
    "        'Mean % Improvement',\n",
    "        'Mean % Improvement'\n",
    "    ]\n",
    "    \n",
    "    for i, (metric, title, y_label) in enumerate(zip(metrics, titles, y_labels)):\n",
    "        ax = axs[i]\n",
    "        \n",
    "        means = []\n",
    "        sems = []\n",
    "        labels = []\n",
    "        counts = []\n",
    "        \n",
    "        for level in level_order:\n",
    "            if level in metrics_data[metric] and len(metrics_data[metric][level]) > 0:\n",
    "                values = metrics_data[metric][level]\n",
    "                means.append(np.mean(values))\n",
    "                sems.append(np.std(values) / np.sqrt(len(values)))\n",
    "                labels.append(level_labels[level])\n",
    "                counts.append(len(values))\n",
    "            else:\n",
    "                means.append(0)\n",
    "                sems.append(0)\n",
    "                labels.append(level_labels.get(level, level))\n",
    "                counts.append(0)\n",
    "        \n",
    "        x_positions = np.arange(len(means)) + 1\n",
    "        \n",
    "        bars = ax.bar(x_positions, means, width=0.6, color=colors[:len(means)], \n",
    "                      yerr=sems, capsize=5, \n",
    "                      error_kw={'ecolor': 'black', 'linewidth': 1, 'capthick': 1})\n",
    "        \n",
    "        for idx, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + sems[idx] + max(means)*0.01,\n",
    "                       f'{height:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.set_title(title, fontsize=12, pad=10)\n",
    "        ax.set_xlabel('Interference Level', fontsize=10)\n",
    "        ax.set_ylabel(y_label, fontsize=10)\n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(labels, rotation=45)\n",
    "        ax.grid(axis='y', linestyle='-', alpha=0.2)\n",
    "        \n",
    "        ax.set_ylim(bottom=0)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    \n",
    "    plt.suptitle(model_name, fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    model_lower = model_name.lower().replace(' ', '_')\n",
    "    png_path = os.path.join(output_dir, f\"{model_lower}_sae_intervention_percentage.png\")\n",
    "    pdf_path = os.path.join(output_dir, f\"{model_lower}_sae_intervention_percentage.pdf\")\n",
    "    \n",
    "    plt.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(pdf_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    statistics = {}\n",
    "    for metric in metrics_data:\n",
    "        statistics[metric] = {}\n",
    "        metric_data = metrics_data[metric]\n",
    "        \n",
    "        for level in metric_data:\n",
    "            if len(metric_data[level]) > 0:\n",
    "                values = metric_data[level]\n",
    "                statistics[metric][level] = {\n",
    "                    'mean_percentage': float(np.mean(values)),\n",
    "                    'std_percentage': float(np.std(values)),\n",
    "                    'sem_percentage': float(np.std(values) / np.sqrt(len(values))),\n",
    "                    'count': len(values),\n",
    "                    'min_percentage': float(np.min(values)),\n",
    "                    'max_percentage': float(np.max(values)),\n",
    "                    'median_percentage': float(np.median(values))\n",
    "                }\n",
    "            else:\n",
    "                statistics[metric][level] = {\n",
    "                    'mean_percentage': 0, 'std_percentage': 0, 'sem_percentage': 0, 'count': 0,\n",
    "                    'min_percentage': 0, 'max_percentage': 0, 'median_percentage': 0\n",
    "                }\n",
    "    \n",
    "    statistics['summary'] = {\n",
    "        'total_test_cases': total_processed,\n",
    "        'total_improvements': total_improved,\n",
    "        'improvement_rate': float(total_improved / total_processed) if total_processed > 0 else 0,\n",
    "        'improvements_per_metric': {metric: sum(len(metrics_data[metric][level]) for level in metrics_data[metric]) \n",
    "                                   for metric in metrics_data}\n",
    "    }\n",
    "    \n",
    "    stats_path = os.path.join(output_dir, f\"{model_lower}_percentage_statistics.json\")\n",
    "    with open(stats_path, 'w') as f:\n",
    "        json.dump(statistics, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved:\")\n",
    "    print(f\"- Plot (PNG): {png_path}\")\n",
    "    print(f\"- Plot (PDF): {pdf_path}\")\n",
    "    print(f\"- Statistics: {stats_path}\")\n",
    "    \n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57461101",
   "metadata": {},
   "source": [
    "## Do a simple visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"results/pythia_gpt2/pythia_sae_intervention_results_0.2_1756957339.json\"\n",
    "try:\n",
    "    stats = plot_sae_intervention_results(\n",
    "        data_path=data_file,\n",
    "        output_dir=\"analysis_results\",\n",
    "        model_name=\"Pythia\"\n",
    "    )\n",
    "    print(\"Plotting completed successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {data_file}\")\n",
    "    print(\"Please check the file path or use the wildcard pattern below.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
