{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575327fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "from collections import Counter\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import *\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "try:\n",
    "    font_path = \"./AvenirLTStd-Roman.otf\"\n",
    "    avenir_font = fm.FontProperties(fname=font_path)\n",
    "    fm.fontManager.addfont(font_path)\n",
    "    mpl.rcParams['font.family'] = avenir_font.get_name()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e903149",
   "metadata": {},
   "source": [
    "## Test Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f77c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_raw_model(model, text):\n",
    "    input_ids = model.to_tokens(text, prepend_bos=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_token_logits = model(input_ids)[0, -1, :]\n",
    "\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # get top10 tokens and their probs\n",
    "    top10_probs, top10_indices = torch.topk(probs, 10)\n",
    "\n",
    "    return top10_indices, top10_probs\n",
    "\n",
    "def run_with_hooked_model(model, text, hook_pos, neuron_id, scale):\n",
    "    def scale_neuron(neuron_id, scale):\n",
    "        def fn_hook(act, hook):\n",
    "            act[:, :, neuron_id] *= scale\n",
    "            return act\n",
    "        return fn_hook\n",
    "\n",
    "    input_ids = model.to_tokens(text, prepend_bos=False)\n",
    "    with torch.no_grad():\n",
    "        last_token_logits = model.run_with_hooks(\n",
    "            input_ids,\n",
    "            fwd_hooks=[\n",
    "                (hook_pos, scale_neuron(neuron_id, scale))\n",
    "            ]\n",
    "        )[0, -1, :]\n",
    "\n",
    "    probs = torch.softmax(last_token_logits, dim=-1)\n",
    "\n",
    "    # get top10 tokens and their probs\n",
    "    top10_probs, top10_tokens = torch.topk(probs, 10)\n",
    "\n",
    "    return top10_tokens, top10_probs\n",
    "\n",
    "def get_test_neurons(neuron_poly_degree, neuron_aligned_clusters):\n",
    "    ranges = [\n",
    "        (1, 1), (2, 5), (6, 20), (21, 50),\n",
    "        (51, 100), (101, 200), (201, 500), (501, float('inf'))\n",
    "    ]\n",
    "    \n",
    "    selected_neurons = {}\n",
    "    for low, high in ranges:\n",
    "        candidates = []\n",
    "        for neuron, count in neuron_poly_degree.items():\n",
    "            clusters = len(neuron_aligned_clusters[neuron])\n",
    "            if low <= clusters <= high:\n",
    "                candidates.append((neuron, clusters))\n",
    "        \n",
    "        if candidates:\n",
    "            selected_neuron, cluster_count = random.choice(candidates)\n",
    "            selected_neurons[f\"{low}-{high}\"] = {\n",
    "                'neuron_id': selected_neuron,\n",
    "                'cluster_count': cluster_count\n",
    "            }\n",
    "    \n",
    "    return selected_neurons\n",
    "\n",
    "def get_test_features(cluster_info, features_data, cluster_ids, num_features=3):\n",
    "    selected_features = []\n",
    "    \n",
    "    for cluster_id in cluster_ids:\n",
    "        feature_ids = cluster_info[cluster_id]['feature_ids']\n",
    "        valid_features = []\n",
    "        \n",
    "        for feature_id in feature_ids:\n",
    "            if feature_id in features_data:\n",
    "                valid_features.append(feature_id)\n",
    "        \n",
    "        if valid_features:\n",
    "            selected = random.sample(valid_features, min(num_features, len(valid_features)))\n",
    "            selected_features.extend(selected)\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "def test_neuron_attack(model, token_vocab, token_embed_mat, features_data, layer_type, layer_index, neuron_id, feature_id):\n",
    "    \"\"\"Test the effect of attacking a specific neuron on token prediction similarities.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        token_vocab: Dictionary containing example sentences for each token\n",
    "        token_embed_mat: Token embedding matrix\n",
    "        features_data: Dictionary containing feature activation data\n",
    "        layer_type: Type of the layer ('att', 'mlp', or 'res')\n",
    "        layer_index: Index of the layer\n",
    "        neuron_id: ID of the neuron to attack\n",
    "        feature_id: ID of the feature being tested\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing maximum changes in similarity under attack\n",
    "    \"\"\"\n",
    "    # Get feature data and most activated token\n",
    "    feature_data = features_data[feature_id]\n",
    "    max_token = get_max_act_token(feature_data)\n",
    "    if max_token is None:\n",
    "        return None\n",
    "    \n",
    "    # Get similar tokens to compare with\n",
    "    similar_tokens = get_similar_tokens(model, max_token, top_k=5, \n",
    "        token_embedding=token_embed_mat[model.to_tokens(max_token, prepend_bos=False)[0, 0].item()])\n",
    "    token_ids = [item[0] for item in similar_tokens]\n",
    "    \n",
    "    # Get test sentences\n",
    "    token_id = model.to_tokens(max_token, prepend_bos=False)[0, 0].item()\n",
    "    if str(token_id) not in token_vocab:\n",
    "        return None\n",
    "    sentences = next(iter(token_vocab[str(token_id)].values()))\n",
    "    \n",
    "    # Setup hook position and test scales\n",
    "    hook_pos = get_hook_position(f'{layer_index}-{layer_type}')\n",
    "    scales = [0, 0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 2.5, 3, 4, 5, 6, 8, 10, 12, 15, 18, 20]\n",
    "    \n",
    "    # Initialize trackers for maximum changes\n",
    "    max_abs_drop = {\n",
    "        'value': 0,\n",
    "        'scale': 0,\n",
    "        'sentence': \"\",\n",
    "        'original_sim': 0,\n",
    "        'attacked_sim': 0\n",
    "    }\n",
    "    max_percent_drop = {\n",
    "        'value': 0,\n",
    "        'percent': 0,\n",
    "        'scale': 0,\n",
    "        'sentence': \"\",\n",
    "        'original_sim': 0,\n",
    "        'attacked_sim': 0\n",
    "    }\n",
    "    max_abs_increase = {\n",
    "        'value': 0,\n",
    "        'scale': 0,\n",
    "        'sentence': \"\",\n",
    "        'original_sim': 0,\n",
    "        'attacked_sim': 0\n",
    "    }\n",
    "    max_percent_increase = {\n",
    "        'value': 0,\n",
    "        'percent': 0,\n",
    "        'scale': 0,\n",
    "        'sentence': \"\",\n",
    "        'original_sim': 0,\n",
    "        'attacked_sim': 0\n",
    "    }\n",
    "    \n",
    "    # Test each sentence\n",
    "    for sentence in sentences:\n",
    "        # Get baseline prediction similarities\n",
    "        raw_top10_tokens, raw_top10_probs = run_with_raw_model(model, sentence)\n",
    "        raw_sim = get_overall_similarity(\n",
    "            token_embed_mat,\n",
    "            raw_top10_tokens,\n",
    "            token_ids,\n",
    "            raw_top10_probs\n",
    "        )\n",
    "        \n",
    "        # Test each scale value\n",
    "        for scale in scales:\n",
    "            # Get prediction similarities under attack\n",
    "            attacked_top10_tokens, attacked_top10_probs = run_with_hooked_model(\n",
    "                model, sentence, hook_pos, neuron_id, scale)\n",
    "            attacked_sim = get_overall_similarity(\n",
    "                token_embed_mat,\n",
    "                attacked_top10_tokens,\n",
    "                token_ids,\n",
    "                attacked_top10_probs\n",
    "            )\n",
    "            \n",
    "            # Calculate changes\n",
    "            sim_change = attacked_sim - raw_sim\n",
    "            if raw_sim > 0:\n",
    "                change_percent = (sim_change / raw_sim) * 100\n",
    "            else:\n",
    "                change_percent = 0\n",
    "            \n",
    "            # Update maximum absolute drop\n",
    "            if sim_change < 0 and abs(sim_change) > abs(max_abs_drop['value']):\n",
    "                max_abs_drop.update({\n",
    "                    'value': float(sim_change),\n",
    "                    'scale': scale,\n",
    "                    'sentence': str(sentence),\n",
    "                    'original_sim': float(raw_sim),\n",
    "                    'attacked_sim': float(attacked_sim)\n",
    "                })\n",
    "            \n",
    "            # Update maximum percentage drop\n",
    "            if sim_change < 0 and abs(change_percent) > abs(max_percent_drop['percent']):\n",
    "                max_percent_drop.update({\n",
    "                    'value': float(sim_change),\n",
    "                    'percent': float(change_percent),\n",
    "                    'scale': scale,\n",
    "                    'sentence': str(sentence),\n",
    "                    'original_sim': float(raw_sim),\n",
    "                    'attacked_sim': float(attacked_sim)\n",
    "                })\n",
    "            \n",
    "            # Update maximum absolute increase\n",
    "            if sim_change > 0 and sim_change > max_abs_increase['value']:\n",
    "                max_abs_increase.update({\n",
    "                    'value': float(sim_change),\n",
    "                    'scale': scale,\n",
    "                    'sentence': str(sentence),\n",
    "                    'original_sim': float(raw_sim),\n",
    "                    'attacked_sim': float(attacked_sim)\n",
    "                })\n",
    "            \n",
    "            # Update maximum percentage increase\n",
    "            if sim_change > 0 and change_percent > max_percent_increase['percent']:\n",
    "                max_percent_increase.update({\n",
    "                    'value': float(sim_change),\n",
    "                    'percent': float(change_percent),\n",
    "                    'scale': scale,\n",
    "                    'sentence': str(sentence),\n",
    "                    'original_sim': float(raw_sim),\n",
    "                    'attacked_sim': float(attacked_sim)\n",
    "                })\n",
    "    \n",
    "    # Return all results\n",
    "    return {\n",
    "        'feature_id': feature_id,\n",
    "        'max_token': max_token,\n",
    "        'max_abs_drop': max_abs_drop,\n",
    "        'max_percent_drop': max_percent_drop,\n",
    "        'max_abs_increase': max_abs_increase,\n",
    "        'max_percent_increase': max_percent_increase\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb832c48",
   "metadata": {},
   "source": [
    "## Pythia Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a889ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./corpus/pythia_vocabulary_sentences.json', 'r') as f:\n",
    "    pythia_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ed92d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m-deduped into HookedTransformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pythia = HookedTransformer.from_pretrained(\n",
    "    model_name='EleutherAI/pythia-70m-deduped',\n",
    "    device='cpu',\n",
    "    local_files_only=True\n",
    ")\n",
    "pythia.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34af1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff5a9ce09734bdd9fd416efecaa4368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817bb27f16284e95a188797bd26bbee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0286b81fe4476a85d2dd14b0e0f120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b26889ee3594a3aa559028403889902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pythia_test_results = defaultdict(lambda: defaultdict(dict))\n",
    "pythia_vocab_mat = pythia.embed.W_E.detach()\n",
    "pythia_semantic_clusters_path = './cluster_plot/pythia-exp'\n",
    "\n",
    "for layer_type in tqdm(['att', 'mlp', 'res']):\n",
    "    for layer_index in tqdm(range(6)):\n",
    "        cluster_info_path = f'{layer_index}-{layer_type}-clusters.json'\n",
    "        with open(os.path.join(pythia_semantic_clusters_path, cluster_info_path), 'r') as f:\n",
    "            cluster_info = json.load(f)\n",
    "\n",
    "        features_data = get_sae_features_by_layer('pythia', layer_type, layer_index)\n",
    "\n",
    "        all_alignment_indices = []\n",
    "        neuron_aligned_clusters = defaultdict(set)\n",
    "\n",
    "        for cluster_id, cluster_data in cluster_info.items():\n",
    "            feature_ids = cluster_data['feature_ids']\n",
    "            \n",
    "            for feature_id in feature_ids:\n",
    "                feature_data = features_data[feature_id]\n",
    "                \n",
    "                for ind, val in zip(\n",
    "                    feature_data['neuron_alignment_indices'],\n",
    "                    feature_data['neuron_alignment_values']\n",
    "                ):\n",
    "                    if val > 0.2:\n",
    "                        all_alignment_indices.append(ind)\n",
    "                        neuron_aligned_clusters[ind].add(cluster_id)\n",
    "\n",
    "        neuron_poly_degree = Counter(all_alignment_indices)\n",
    "        neuron_poly_degree = dict(sorted(neuron_poly_degree.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        test_neurons = get_test_neurons(neuron_poly_degree, neuron_aligned_clusters)\n",
    "        \n",
    "        for degree_range, neuron_info in test_neurons.items():\n",
    "            neuron_id = neuron_info['neuron_id']\n",
    "            connected_clusters = list(neuron_aligned_clusters[neuron_id])\n",
    "            \n",
    "            if len(connected_clusters) > 3:\n",
    "                test_clusters = random.sample(connected_clusters, 3)\n",
    "            else:\n",
    "                test_clusters = connected_clusters\n",
    "            \n",
    "            test_features = get_test_features(cluster_info, features_data, test_clusters)\n",
    "            \n",
    "            attack_results = []\n",
    "            for feature_id in test_features:\n",
    "                result = test_neuron_attack(\n",
    "                    pythia, pythia_vocab,\n",
    "                    pythia_vocab_mat,\n",
    "                    features_data,\n",
    "                    layer_type, layer_index,\n",
    "                    neuron_id, feature_id\n",
    "                )\n",
    "                if result is not None:\n",
    "                    attack_results.append(result)\n",
    "            \n",
    "            pythia_test_results[f\"{layer_type}_{layer_index}\"][degree_range] = {\n",
    "                'neuron_id': neuron_id,\n",
    "                'cluster_count': neuron_info['cluster_count'],\n",
    "                'attack_results': attack_results\n",
    "            }\n",
    "\n",
    "with open('./pythia_neuron_attack_results.json', 'w') as f:\n",
    "    json.dump(pythia_test_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7c0b2",
   "metadata": {},
   "source": [
    "## GPT2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5061048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "gpt2 = HookedTransformer.from_pretrained(\n",
    "    model_name='gpt2-small',\n",
    "    device='cpu',\n",
    "    local_files_only=True\n",
    ")\n",
    "gpt2.eval()\n",
    "\n",
    "with open('./corpus/gpt2_vocabulary_sentences.json', 'r') as f:\n",
    "    gpt2_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281b349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06c053377f84a25ace87daeb65c3c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f779896080004a4e81553f91b0fddc52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa4f63ca771402896590c8c64a42fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfff20d04a94a519143210fcdd321d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1487d29f3dae45d6afd237c93894b049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2_test_results = defaultdict(lambda: defaultdict(dict))\n",
    "gpt2_vocab_mat = gpt2.embed.W_E.detach()\n",
    "gpt2_semantic_clusters_path = './cluster_plot/gpt2-exp'\n",
    "\n",
    "for layer_type in tqdm(['att', 'res_mid', 'mlp', 'res_post']):\n",
    "    for layer_index in tqdm(range(12)):\n",
    "        \n",
    "        cluster_info_path = f'{layer_index}-{layer_type}-clusters.json'\n",
    "        with open(os.path.join(gpt2_semantic_clusters_path, cluster_info_path), 'r') as f:\n",
    "            cluster_info = json.load(f)\n",
    "\n",
    "        features_data = get_sae_features_by_layer('gpt2', layer_type, layer_index)\n",
    "\n",
    "        all_alignment_indices = []\n",
    "        neuron_aligned_clusters = defaultdict(set)\n",
    "\n",
    "        for cluster_id, cluster_data in cluster_info.items():\n",
    "            feature_ids = cluster_data['feature_ids']\n",
    "            \n",
    "            for feature_id in feature_ids:\n",
    "                feature_data = features_data[feature_id]\n",
    "                \n",
    "                for ind, val in zip(\n",
    "                    feature_data['neuron_alignment_indices'],\n",
    "                    feature_data['neuron_alignment_values']\n",
    "                ):\n",
    "                    if val > 0.2:\n",
    "                        all_alignment_indices.append(ind)\n",
    "                        neuron_aligned_clusters[ind].add(cluster_id)\n",
    "\n",
    "        neuron_poly_degree = Counter(all_alignment_indices)\n",
    "        neuron_poly_degree = dict(sorted(neuron_poly_degree.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        test_neurons = get_test_neurons(neuron_poly_degree, neuron_aligned_clusters)\n",
    "        \n",
    "        for degree_range, neuron_info in test_neurons.items():\n",
    "            neuron_id = neuron_info['neuron_id']\n",
    "            connected_clusters = list(neuron_aligned_clusters[neuron_id])\n",
    "            \n",
    "            if len(connected_clusters) > 3:\n",
    "                test_clusters = random.sample(connected_clusters, 3)\n",
    "            else:\n",
    "                test_clusters = connected_clusters\n",
    "            \n",
    "            test_features = get_test_features(cluster_info, features_data, test_clusters)\n",
    "            \n",
    "            attack_results = []\n",
    "            for feature_id in test_features:\n",
    "                result = test_neuron_attack(\n",
    "                    gpt2, gpt2_vocab,\n",
    "                    gpt2_vocab_mat,\n",
    "                    features_data,\n",
    "                    layer_type, layer_index,\n",
    "                    neuron_id, feature_id\n",
    "                )\n",
    "                if result is not None:\n",
    "                    attack_results.append(result)\n",
    "            \n",
    "            gpt2_test_results[f\"{layer_type}_{layer_index}\"][degree_range] = {\n",
    "                'neuron_id': neuron_id,\n",
    "                'cluster_count': neuron_info['cluster_count'],\n",
    "                'attack_results': attack_results\n",
    "            }\n",
    "\n",
    "with open('./gpt2_neuron_attack_results.json', 'w') as f:\n",
    "    json.dump(gpt2_test_results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
