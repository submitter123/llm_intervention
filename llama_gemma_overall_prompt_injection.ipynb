{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0477ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Import packages for large model experiments (Updated)\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "\n",
    "# Import model utilities and sentences\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('__file__')))\n",
    "from utils.utils_model import get_hooked_pythia_70m, get_hooked_gpt2_small, get_llama_3_8B, get_gemma_2_9B\n",
    "from utils.utils_data import load_type_dicts, get_token_id_type, get_token_str_type\n",
    "from corpus.type_sentences import SENTENCES_OF_TYPE\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Parameters\n",
    "PYTHIA_HIGH_INTERFERENCE_THRESHOLD = 0.5\n",
    "GPT2_HIGH_INTERFERENCE_THRESHOLD = 0.5\n",
    "BASELINE_INTERFERENCE_THRESHOLD = 0.2\n",
    "N_TRIALS = 100\n",
    "N_TEST_SENTENCES = 100\n",
    "TOP_K = 30\n",
    "INJECTION_SIZE = 30\n",
    "N_SELECTED_ACT_TOKENS = 10  # Number of high_activation_tokens to select per interference feature\n",
    "TOKEN_SET_TYPE = 'union'\n",
    "RANDOM_SEED = 42\n",
    "MED_DATA_DIR = Path(\"./med_data\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Parameters set with thresholds: Pythia={PYTHIA_HIGH_INTERFERENCE_THRESHOLD}, GPT-2={GPT2_HIGH_INTERFERENCE_THRESHOLD}\")\n",
    "print(f\"N_SELECTED_ACT_TOKENS: {N_SELECTED_ACT_TOKENS}\")\n",
    "print(f\"TOKEN_SET_TYPE: {TOKEN_SET_TYPE}\")\n",
    "print(f\"Available sentence types: {list(SENTENCES_OF_TYPE.keys())}\")\n",
    "for token_type, sentences in SENTENCES_OF_TYPE.items():\n",
    "    print(f\"  {token_type}: {len(sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f02b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define functions to generate both union and overlap token sets\n",
    "def generate_union_token_sets(token_type: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate union token sets from Pythia and GPT-2 interference data\n",
    "    First remove high/medium overlap within each model, then take unions\n",
    "    \n",
    "    Args:\n",
    "        token_type: token type name\n",
    "    \n",
    "    Returns:\n",
    "        Dict with union token sets for large model testing\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING UNION TOKEN SETS FOR {token_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load token type mappings\n",
    "    pythia_token_id_to_type, pythia_token_str_to_type = load_type_dicts('pythia')\n",
    "    gpt2_token_id_to_type, gpt2_token_str_to_type = load_type_dicts('gpt2')\n",
    "    \n",
    "    # Load models first for vocabulary extraction\n",
    "    pythia_model = get_hooked_pythia_70m(device)\n",
    "    gpt2_model = get_hooked_gpt2_small(device)\n",
    "    \n",
    "    # Initialize token sets for both models\n",
    "    pythia_target = set()\n",
    "    pythia_high_interference = set()\n",
    "    pythia_medium_interference = set()\n",
    "    gpt2_target = set()\n",
    "    gpt2_high_interference = set()\n",
    "    gpt2_medium_interference = set()\n",
    "    \n",
    "    # Process Pythia data\n",
    "    print(f\"Processing Pythia {token_type} data...\")\n",
    "    pythia_file = MED_DATA_DIR / f\"pythia_{token_type}_tokens_r0.8_i0.2_s0.3.json\"\n",
    "    if pythia_file.exists():\n",
    "        with open(pythia_file, 'r', encoding='utf-8') as f:\n",
    "            pythia_data = json.load(f)\n",
    "        \n",
    "        # Extract target tokens from vocabulary\n",
    "        for token_str, token_type_label in pythia_token_str_to_type.items():\n",
    "            if token_type_label == token_type:\n",
    "                pythia_target.add(token_str)\n",
    "        \n",
    "        # Extract interference tokens\n",
    "        for layer_type, layers in pythia_data.items():\n",
    "            if layer_type == 'summary' or not isinstance(layers, dict):\n",
    "                continue\n",
    "            \n",
    "            for layer_idx, layer_data in layers.items():\n",
    "                if not isinstance(layer_data, dict):\n",
    "                    continue\n",
    "                \n",
    "                # Process target features\n",
    "                # target_features = layer_data.get('target_features', [])\n",
    "                # for feature in target_features:\n",
    "                #     high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                #     for token_str in high_activation_tokens:\n",
    "                #         token_type_label = get_token_str_type('pythia', token_str, pythia_token_str_to_type)\n",
    "                #         if token_type_label != token_type:\n",
    "                #             pythia_high_interference.add(token_str)\n",
    "                \n",
    "                # Process interference features\n",
    "                interference_features = layer_data.get('interference_features', [])\n",
    "                for feature in interference_features:\n",
    "                    interferences = feature.get('interferences', [])\n",
    "                    if not interferences:\n",
    "                        continue\n",
    "                    \n",
    "                    max_interference_value = max(\n",
    "                        interference['interference_value'] for interference in interferences\n",
    "                    )\n",
    "                    \n",
    "                    high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                    \n",
    "                    if max_interference_value >= PYTHIA_HIGH_INTERFERENCE_THRESHOLD:\n",
    "                        pythia_high_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "                    elif max_interference_value >= BASELINE_INTERFERENCE_THRESHOLD:\n",
    "                        pythia_medium_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "        \n",
    "        print(f\"  Pythia before dedup - Target: {len(pythia_target)}, High: {len(pythia_high_interference)}, Medium: {len(pythia_medium_interference)}\")\n",
    "    \n",
    "    # Process GPT-2 data\n",
    "    print(f\"Processing GPT-2 {token_type} data...\")\n",
    "    gpt2_file = MED_DATA_DIR / f\"gpt2_{token_type}_tokens_r0.8_i0.2_s0.3.json\"\n",
    "    if gpt2_file.exists():\n",
    "        with open(gpt2_file, 'r', encoding='utf-8') as f:\n",
    "            gpt2_data = json.load(f)\n",
    "        \n",
    "        # Extract target tokens from vocabulary\n",
    "        for token_str, token_type_label in gpt2_token_str_to_type.items():\n",
    "            if token_type_label == token_type:\n",
    "                gpt2_target.add(token_str)\n",
    "        \n",
    "        # Extract interference tokens\n",
    "        for layer_type, layers in gpt2_data.items():\n",
    "            if layer_type == 'summary' or not isinstance(layers, dict):\n",
    "                continue\n",
    "            \n",
    "            for layer_idx, layer_data in layers.items():\n",
    "                if not isinstance(layer_data, dict):\n",
    "                    continue\n",
    "                \n",
    "                # Process target features\n",
    "                # target_features = layer_data.get('target_features', [])\n",
    "                # for feature in target_features:\n",
    "                #     high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                #     for token_str in high_activation_tokens:\n",
    "                #         token_type_label = get_token_str_type('gpt2', token_str, gpt2_token_str_to_type)\n",
    "                #         if token_type_label != token_type:\n",
    "                #             gpt2_high_interference.add(token_str)\n",
    "                \n",
    "                # Process interference features\n",
    "                interference_features = layer_data.get('interference_features', [])\n",
    "                for feature in interference_features:\n",
    "                    interferences = feature.get('interferences', [])\n",
    "                    if not interferences:\n",
    "                        continue\n",
    "                    \n",
    "                    max_interference_value = max(\n",
    "                        interference['interference_value'] for interference in interferences\n",
    "                    )\n",
    "                    \n",
    "                    high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                    \n",
    "                    if max_interference_value >= GPT2_HIGH_INTERFERENCE_THRESHOLD:\n",
    "                        gpt2_high_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "                    elif max_interference_value >= BASELINE_INTERFERENCE_THRESHOLD:\n",
    "                        gpt2_medium_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "        \n",
    "        print(f\"  GPT-2 before dedup - Target: {len(gpt2_target)}, High: {len(gpt2_high_interference)}, Medium: {len(gpt2_medium_interference)}\")\n",
    "    \n",
    "    # STEP 1: Remove high/medium overlap WITHIN EACH MODEL\n",
    "    print(f\"\\nSTEP 1: Removing high/medium overlaps within each model...\")\n",
    "    \n",
    "    # Remove Pythia high/medium overlap (keep in high)\n",
    "    pythia_high_medium_overlap = pythia_high_interference & pythia_medium_interference\n",
    "    print(f\"  Pythia high/medium overlap: {len(pythia_high_medium_overlap)} tokens\")\n",
    "    \n",
    "    pythia_high_interference_clean = pythia_high_interference - pythia_high_medium_overlap\n",
    "    pythia_medium_interference_clean = pythia_medium_interference - pythia_high_medium_overlap\n",
    "    \n",
    "    # Remove GPT-2 high/medium overlap (keep in high)\n",
    "    gpt2_high_medium_overlap = gpt2_high_interference & gpt2_medium_interference\n",
    "    print(f\"  GPT-2 high/medium overlap: {len(gpt2_high_medium_overlap)} tokens\")\n",
    "    \n",
    "    gpt2_high_interference_clean = gpt2_high_interference - gpt2_high_medium_overlap\n",
    "    gpt2_medium_interference_clean = gpt2_medium_interference - gpt2_high_medium_overlap\n",
    "    \n",
    "    print(f\"  After within-model dedup:\")\n",
    "    print(f\"    Pythia - High: {len(pythia_high_interference_clean)}, Medium: {len(pythia_medium_interference_clean)}\")\n",
    "    print(f\"    GPT-2 - High: {len(gpt2_high_interference_clean)}, Medium: {len(gpt2_medium_interference_clean)}\")\n",
    "    \n",
    "    # Calculate random sets for each model (using original sets before dedup)\n",
    "    print(f\"\\nCalculating individual random sets...\")\n",
    "    \n",
    "    # Get all vocabulary tokens\n",
    "    all_pythia_tokens = set()\n",
    "    for token_id in range(pythia_model.cfg.d_vocab):\n",
    "        try:\n",
    "            token_str = pythia_model.to_string(token_id)\n",
    "            if token_str and token_str.strip():\n",
    "                all_pythia_tokens.add(token_str)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    all_gpt2_tokens = set()\n",
    "    for token_id in range(gpt2_model.cfg.d_vocab):\n",
    "        try:\n",
    "            token_str = gpt2_model.to_string(token_id)\n",
    "            if token_str and token_str.strip():\n",
    "                all_gpt2_tokens.add(token_str)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Calculate random sets for each model (using original interference sets)\n",
    "    pythia_used_tokens = pythia_target | pythia_high_interference | pythia_medium_interference\n",
    "    pythia_random = all_pythia_tokens - pythia_used_tokens\n",
    "    \n",
    "    gpt2_used_tokens = gpt2_target | gpt2_high_interference | gpt2_medium_interference\n",
    "    gpt2_random = all_gpt2_tokens - gpt2_used_tokens\n",
    "    \n",
    "    print(f\"  Pythia random: {len(pythia_random)} tokens\")\n",
    "    print(f\"  GPT-2 random: {len(gpt2_random)} tokens\")\n",
    "    \n",
    "    # STEP 2: Calculate unions of cleaned sets\n",
    "    print(f\"\\nSTEP 2: Calculating unions of cleaned interference sets...\")\n",
    "    \n",
    "    # Get unions of cleaned sets\n",
    "    union_high_interference = pythia_high_interference_clean | gpt2_high_interference_clean\n",
    "    union_medium_interference = pythia_medium_interference_clean | gpt2_medium_interference_clean\n",
    "    target_union = pythia_target | gpt2_target\n",
    "    random_union = pythia_random | gpt2_random\n",
    "    \n",
    "    # Convert to lists and filter\n",
    "    token_sets = {\n",
    "        'target': [t for t in target_union if t and t.strip()],\n",
    "        'high_interference': [t for t in union_high_interference if t and t.strip()],\n",
    "        'medium_interference': [t for t in union_medium_interference if t and t.strip()],\n",
    "        'random': [t for t in random_union if t and t.strip()]\n",
    "    }\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"\\nFinal union token sets:\")\n",
    "    for category, tokens in token_sets.items():\n",
    "        print(f\"  {category}: {len(tokens)} tokens\")\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = MED_DATA_DIR / f\"union_{token_type}_token_sets_{N_SELECTED_ACT_TOKENS}tk_{PYTHIA_HIGH_INTERFERENCE_THRESHOLD}ph_{GPT2_HIGH_INTERFERENCE_THRESHOLD}gh.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(token_sets, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nSaved union token sets to: {output_file}\")\n",
    "    \n",
    "    # Clear models from memory\n",
    "    del pythia_model, gpt2_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return token_sets\n",
    "\n",
    "def generate_overlap_token_sets(token_type: str) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Generate overlap token sets from Pythia and GPT-2 interference data\n",
    "    Use intersections for interference sets, unions for target/random\n",
    "    \n",
    "    Args:\n",
    "        token_type: token type name\n",
    "    \n",
    "    Returns:\n",
    "        Dict with overlap token sets for large model testing\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GENERATING OVERLAP TOKEN SETS FOR {token_type.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load token type mappings\n",
    "    pythia_token_id_to_type, pythia_token_str_to_type = load_type_dicts('pythia')\n",
    "    gpt2_token_id_to_type, gpt2_token_str_to_type = load_type_dicts('gpt2')\n",
    "    \n",
    "    # Load models first for vocabulary extraction\n",
    "    pythia_model = get_hooked_pythia_70m(device)\n",
    "    gpt2_model = get_hooked_gpt2_small(device)\n",
    "    \n",
    "    # Initialize token sets for both models\n",
    "    pythia_target = set()\n",
    "    pythia_high_interference = set()\n",
    "    pythia_medium_interference = set()\n",
    "    gpt2_target = set()\n",
    "    gpt2_high_interference = set()\n",
    "    gpt2_medium_interference = set()\n",
    "    \n",
    "    # Process Pythia data\n",
    "    print(f\"Processing Pythia {token_type} data...\")\n",
    "    pythia_file = MED_DATA_DIR / f\"pythia_{token_type}_tokens_r0.8_i0.2_s0.3.json\"\n",
    "    if pythia_file.exists():\n",
    "        with open(pythia_file, 'r', encoding='utf-8') as f:\n",
    "            pythia_data = json.load(f)\n",
    "        \n",
    "        # Extract target tokens from vocabulary\n",
    "        for token_str, token_type_label in pythia_token_str_to_type.items():\n",
    "            if token_type_label == token_type:\n",
    "                pythia_target.add(token_str)\n",
    "        \n",
    "        # Extract interference tokens (NO within-model deduplication)\n",
    "        for layer_type, layers in pythia_data.items():\n",
    "            if layer_type == 'summary' or not isinstance(layers, dict):\n",
    "                continue\n",
    "            \n",
    "            for layer_idx, layer_data in layers.items():\n",
    "                if not isinstance(layer_data, dict):\n",
    "                    continue\n",
    "                \n",
    "                # Process target features\n",
    "                target_features = layer_data.get('target_features', [])\n",
    "                for feature in target_features:\n",
    "                    high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                    for token_str in high_activation_tokens[:N_SELECTED_ACT_TOKENS]:\n",
    "                        token_type_label = get_token_str_type('pythia', token_str, pythia_token_str_to_type)\n",
    "                        if token_type_label != token_type:\n",
    "                            pythia_high_interference.add(token_str)\n",
    "                \n",
    "                # Process interference features\n",
    "                interference_features = layer_data.get('interference_features', [])\n",
    "                for feature in interference_features:\n",
    "                    interferences = feature.get('interferences', [])\n",
    "                    if not interferences:\n",
    "                        continue\n",
    "                    \n",
    "                    max_interference_value = max(\n",
    "                        interference['interference_value'] for interference in interferences\n",
    "                    )\n",
    "                    \n",
    "                    high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                    \n",
    "                    if max_interference_value >= PYTHIA_HIGH_INTERFERENCE_THRESHOLD:\n",
    "                        pythia_high_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "                    elif max_interference_value >= BASELINE_INTERFERENCE_THRESHOLD:\n",
    "                        pythia_medium_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "        \n",
    "        print(f\"  Pythia - Target: {len(pythia_target)}, High: {len(pythia_high_interference)}, Medium: {len(pythia_medium_interference)}\")\n",
    "    \n",
    "    # Process GPT-2 data\n",
    "    print(f\"Processing GPT-2 {token_type} data...\")\n",
    "    gpt2_file = MED_DATA_DIR / f\"gpt2_{token_type}_tokens_r0.8_i0.2_s0.3.json\"\n",
    "    if gpt2_file.exists():\n",
    "        with open(gpt2_file, 'r', encoding='utf-8') as f:\n",
    "            gpt2_data = json.load(f)\n",
    "        \n",
    "        # Extract target tokens from vocabulary\n",
    "        for token_str, token_type_label in gpt2_token_str_to_type.items():\n",
    "            if token_type_label == token_type:\n",
    "                gpt2_target.add(token_str)\n",
    "        \n",
    "        # Extract interference tokens (NO within-model deduplication)\n",
    "        for layer_type, layers in gpt2_data.items():\n",
    "            if layer_type == 'summary' or not isinstance(layers, dict):\n",
    "                continue\n",
    "            \n",
    "            for layer_idx, layer_data in layers.items():\n",
    "                if not isinstance(layer_data, dict):\n",
    "                    continue\n",
    "                \n",
    "                # Process target features\n",
    "                target_features = layer_data.get('target_features', [])\n",
    "                for feature in target_features:\n",
    "                    high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                    for token_str in high_activation_tokens[:N_SELECTED_ACT_TOKENS]:\n",
    "                        token_type_label = get_token_str_type('gpt2', token_str, gpt2_token_str_to_type)\n",
    "                        if token_type_label != token_type:\n",
    "                            gpt2_high_interference.add(token_str)\n",
    "                \n",
    "                # Process interference features\n",
    "                interference_features = layer_data.get('interference_features', [])\n",
    "                for feature in interference_features:\n",
    "                    interferences = feature.get('interferences', [])\n",
    "                    if not interferences:\n",
    "                        continue\n",
    "                    \n",
    "                    max_interference_value = max(\n",
    "                        interference['interference_value'] for interference in interferences\n",
    "                    )\n",
    "                    \n",
    "                    high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                    \n",
    "                    if max_interference_value >= GPT2_HIGH_INTERFERENCE_THRESHOLD:\n",
    "                        gpt2_high_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "                    elif max_interference_value >= BASELINE_INTERFERENCE_THRESHOLD:\n",
    "                        gpt2_medium_interference.update(high_activation_tokens[:N_SELECTED_ACT_TOKENS])\n",
    "        \n",
    "        print(f\"  GPT-2 - Target: {len(gpt2_target)}, High: {len(gpt2_high_interference)}, Medium: {len(gpt2_medium_interference)}\")\n",
    "    \n",
    "    # Calculate random sets for each model\n",
    "    print(f\"\\nCalculating individual random sets...\")\n",
    "    \n",
    "    # Get all vocabulary tokens\n",
    "    all_pythia_tokens = set()\n",
    "    for token_id in range(pythia_model.cfg.d_vocab):\n",
    "        try:\n",
    "            token_str = pythia_model.to_string(token_id)\n",
    "            if token_str and token_str.strip():\n",
    "                all_pythia_tokens.add(token_str)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    all_gpt2_tokens = set()\n",
    "    for token_id in range(gpt2_model.cfg.d_vocab):\n",
    "        try:\n",
    "            token_str = gpt2_model.to_string(token_id)\n",
    "            if token_str and token_str.strip():\n",
    "                all_gpt2_tokens.add(token_str)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Calculate random sets: all_tokens - all_used_tokens\n",
    "    pythia_used_tokens = pythia_target | pythia_high_interference | pythia_medium_interference\n",
    "    pythia_random = all_pythia_tokens - pythia_used_tokens\n",
    "    \n",
    "    gpt2_used_tokens = gpt2_target | gpt2_high_interference | gpt2_medium_interference\n",
    "    gpt2_random = all_gpt2_tokens - gpt2_used_tokens\n",
    "    \n",
    "    print(f\"  Pythia random: {len(pythia_random)} tokens\")\n",
    "    print(f\"  GPT-2 random: {len(gpt2_random)} tokens\")\n",
    "    \n",
    "    # Calculate overlaps and unions\n",
    "    print(f\"\\nCalculating overlaps and unions...\")\n",
    "    \n",
    "    # Target and random: unions\n",
    "    target_union = pythia_target | gpt2_target\n",
    "    random_overlap = pythia_random & gpt2_random  # intersection for random\n",
    "    \n",
    "    # High and medium interference: intersections\n",
    "    overlap_high_interference = pythia_high_interference & gpt2_high_interference\n",
    "    overlap_medium_interference = pythia_medium_interference & gpt2_medium_interference\n",
    "    \n",
    "    print(f\"  Target union: {len(target_union)} tokens\")\n",
    "    print(f\"  High interference overlap: {len(overlap_high_interference)} tokens\")\n",
    "    print(f\"  Medium interference overlap: {len(overlap_medium_interference)} tokens\")\n",
    "    print(f\"  Random overlap: {len(random_overlap)} tokens\")\n",
    "    \n",
    "    # Remove overlap between high and medium (keep in high)\n",
    "    print(f\"\\nRemoving high/medium overlap...\")\n",
    "    high_medium_overlap = overlap_high_interference & overlap_medium_interference\n",
    "    print(f\"  High-Medium overlap to remove: {len(high_medium_overlap)} tokens\")\n",
    "    \n",
    "    final_overlap_high = overlap_high_interference - high_medium_overlap\n",
    "    final_overlap_medium = overlap_medium_interference - high_medium_overlap\n",
    "    \n",
    "    # Convert to lists and filter\n",
    "    token_sets = {\n",
    "        'target': [t for t in target_union if t and t.strip()],\n",
    "        'high_interference': [t for t in final_overlap_high if t and t.strip()],\n",
    "        'medium_interference': [t for t in final_overlap_medium if t and t.strip()],\n",
    "        'random': [t for t in random_overlap if t and t.strip()]\n",
    "    }\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"\\nFinal overlap token sets:\")\n",
    "    for category, tokens in token_sets.items():\n",
    "        print(f\"  {category}: {len(tokens)} tokens\")\n",
    "    \n",
    "    # Save to file\n",
    "    output_file = MED_DATA_DIR / f\"overlap_{token_type}_token_sets_{N_SELECTED_ACT_TOKENS}tk_{PYTHIA_HIGH_INTERFERENCE_THRESHOLD}ph_{GPT2_HIGH_INTERFERENCE_THRESHOLD}gh.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(token_sets, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nSaved overlap token sets to: {output_file}\")\n",
    "    \n",
    "    # Clear models from memory\n",
    "    del pythia_model, gpt2_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return token_sets\n",
    "\n",
    "def load_token_sets(token_type: str, set_type: str = 'union') -> Optional[Dict[str, List[str]]]:\n",
    "    \"\"\"Load pre-generated token sets\"\"\"\n",
    "    filename = f\"{set_type}_{token_type}_token_sets_{N_SELECTED_ACT_TOKENS}tk_{PYTHIA_HIGH_INTERFERENCE_THRESHOLD}ph_{GPT2_HIGH_INTERFERENCE_THRESHOLD}gh.json\"\n",
    "    token_sets_file = MED_DATA_DIR / filename\n",
    "    if token_sets_file.exists():\n",
    "        with open(token_sets_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c774a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Generate token sets for all types (Updated)\n",
    "print(f\"Generating {TOKEN_SET_TYPE} token sets for all token types...\")\n",
    "\n",
    "token_sets = {}\n",
    "token_types = [\n",
    "    'location', 'person', 'emotion', 'color', 'animal', 'number', 'science', 'time',\n",
    "]\n",
    "\n",
    "for token_type in token_types:\n",
    "    try:\n",
    "        # Try to load existing sets first\n",
    "        existing_sets = load_token_sets(token_type, TOKEN_SET_TYPE)\n",
    "        if existing_sets:\n",
    "            print(f\"\\nLoaded existing {TOKEN_SET_TYPE} sets for {token_type}\")\n",
    "            token_sets[token_type] = existing_sets\n",
    "        else:\n",
    "            # Generate new sets\n",
    "            if TOKEN_SET_TYPE == 'union':\n",
    "                generated_sets = generate_union_token_sets(token_type)\n",
    "            elif TOKEN_SET_TYPE == 'overlap':\n",
    "                generated_sets = generate_overlap_token_sets(token_type)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown TOKEN_SET_TYPE: {TOKEN_SET_TYPE}\")\n",
    "            \n",
    "            token_sets[token_type] = generated_sets\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {token_type}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{TOKEN_SET_TYPE.upper()} TOKEN SETS GENERATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Print summary\n",
    "for token_type, token_set in token_sets.items():\n",
    "    print(f\"\\n{token_type.upper()}:\")\n",
    "    for category, tokens in token_set.items():\n",
    "        print(f\"  {category}: {len(tokens)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f9421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Define utility functions for large model experiments (Updated with large model token type checking)\n",
    "from utils.utils_data import get_large_model_token_type, load_large_model_token_type_dict\n",
    "LARGE_MODEL_TOKEN_TYPE_PATH = \"./dataset/large_model_token_type.json\"\n",
    "large_model_token_type_dict = load_large_model_token_type_dict(LARGE_MODEL_TOKEN_TYPE_PATH)\n",
    "\n",
    "def inject_tokens_large_model(sentence: str, tokens: List[str]) -> str:\n",
    "    \"\"\"Inject tokens at the beginning of sentence with parentheses\"\"\"\n",
    "    return f\"({''.join(tokens)}){sentence}\"\n",
    "\n",
    "def get_next_token_probs_large_model(model, tokenizer, text: str, top_k: int = 10) -> List[Tuple[int, str, float]]:\n",
    "    \"\"\"Get top-k next token predictions with probabilities for large models\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, prob in zip(top_k_indices, top_k_probs):\n",
    "        try:\n",
    "            token_str = tokenizer.decode([idx.item()], skip_special_tokens=True)\n",
    "            results.append((idx.item(), token_str, prob.item()))\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def check_token_type_from_small_models(\n",
    "    token_str: str, \n",
    "    target_type: str,\n",
    "    pythia_token_str_to_type: Dict[str, str],\n",
    "    gpt2_token_str_to_type: Dict[str, str]\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if a token string is of target type based on Pythia/GPT-2 mappings\n",
    "    \"\"\"\n",
    "    # Check in Pythia mapping\n",
    "    pythia_type = get_token_str_type('pythia', token_str, pythia_token_str_to_type)\n",
    "    if pythia_type == target_type:\n",
    "        return True\n",
    "    # Check in GPT-2 mapping\n",
    "    gpt2_type = get_token_str_type('gpt2', token_str, gpt2_token_str_to_type)\n",
    "    if gpt2_type == target_type:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_token_type_from_large_model(\n",
    "    token_str: str,\n",
    "    target_type: str,\n",
    "    large_model_token_type_dict: dict\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Check if a token string is of target type based on large model token type dict\n",
    "    \"\"\"\n",
    "    token_type = get_large_model_token_type(token_str, large_model_token_type_dict)\n",
    "    return token_type == target_type\n",
    "\n",
    "def run_injection_experiment_large_model_with_type_check(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sentence: str,\n",
    "    injection_tokens: List[str],\n",
    "    target_type: str,\n",
    "    pythia_token_str_to_type: Dict[str, str],\n",
    "    gpt2_token_str_to_type: Dict[str, str],\n",
    "    n_trials: int = N_TRIALS,\n",
    "    top_k: int = TOP_K,\n",
    "    injection_size: int = INJECTION_SIZE,\n",
    "    use_large_model_type: bool = False,\n",
    "    large_model_token_type_dict: dict = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run injection experiments for large models with token type checking\n",
    "    Args:\n",
    "        use_large_model_type: whether to use large model token type dict for checking\n",
    "        large_model_token_type_dict: large model token type dict\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'count_increases': 0,\n",
    "        'prob_increases': 0,\n",
    "        'total_trials': n_trials,\n",
    "        'detailed_metrics': []\n",
    "    }\n",
    "    # Get original predictions\n",
    "    original_top_k = get_next_token_probs_large_model(model, tokenizer, sentence, top_k)\n",
    "    original_ids = [t[0] for t in original_top_k]\n",
    "    original_strs = [t[1] for t in original_top_k]\n",
    "    original_probs = [t[2] for t in original_top_k]\n",
    "    # Count original target tokens using type checking\n",
    "    original_count = 0\n",
    "    original_prob = 0.0\n",
    "    for token_str, prob in zip(original_strs, original_probs):\n",
    "        if use_large_model_type:\n",
    "            is_target = check_token_type_from_large_model(token_str, target_type, large_model_token_type_dict)\n",
    "        else:\n",
    "            is_target = check_token_type_from_small_models(\n",
    "                token_str, target_type, pythia_token_str_to_type, gpt2_token_str_to_type\n",
    "            )\n",
    "        if is_target:\n",
    "            original_count += 1\n",
    "            original_prob += prob\n",
    "    original_count = float(original_count)\n",
    "    original_prob = float(original_prob)\n",
    "    # Run injection trials\n",
    "    for trial in range(n_trials):\n",
    "        try:\n",
    "            # Sample injection tokens\n",
    "            if len(injection_tokens) >= injection_size:\n",
    "                selected_tokens = random.sample(injection_tokens, injection_size)\n",
    "            else:\n",
    "                selected_tokens = random.choices(injection_tokens, k=injection_size)\n",
    "            # Create injected sentence\n",
    "            injected_sentence = inject_tokens_large_model(sentence, selected_tokens)\n",
    "            # Get predictions\n",
    "            injected_top_k = get_next_token_probs_large_model(model, tokenizer, injected_sentence, top_k)\n",
    "            injected_ids = [t[0] for t in injected_top_k]\n",
    "            injected_strs = [t[1] for t in injected_top_k]\n",
    "            injected_probs = [t[2] for t in injected_top_k]\n",
    "            # Count injected target tokens using type checking\n",
    "            injected_count = 0\n",
    "            injected_prob = 0.0\n",
    "            for token_str, prob in zip(injected_strs, injected_probs):\n",
    "                if use_large_model_type:\n",
    "                    is_target = check_token_type_from_large_model(token_str, target_type, large_model_token_type_dict)\n",
    "                else:\n",
    "                    is_target = check_token_type_from_small_models(\n",
    "                        token_str, target_type, pythia_token_str_to_type, gpt2_token_str_to_type\n",
    "                    )\n",
    "                if is_target:\n",
    "                    injected_count += 1\n",
    "                    injected_prob += prob\n",
    "            injected_count = float(injected_count)\n",
    "            injected_prob = float(injected_prob)\n",
    "            # Record trial data\n",
    "            trial_data = {\n",
    "                'trial': trial,\n",
    "                'selected_tokens': selected_tokens,\n",
    "                'original_count': original_count,\n",
    "                'injected_count': injected_count,\n",
    "                'original_prob': original_prob,\n",
    "                'injected_prob': injected_prob,\n",
    "                'count_increase': bool(injected_count > original_count),\n",
    "                'prob_increase': bool(injected_prob > original_prob),\n",
    "                'original_predictions': [(id, s, p) for id, s, p in zip(original_ids, original_strs, original_probs)],\n",
    "                'injected_predictions': [(id, s, p) for id, s, p in zip(injected_ids, injected_strs, injected_probs)]\n",
    "            }\n",
    "            results['detailed_metrics'].append(trial_data)\n",
    "            if injected_count > original_count:\n",
    "                results['count_increases'] += 1\n",
    "            if injected_prob > original_prob:\n",
    "                results['prob_increases'] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"    Error in trial {trial}: {e}\")\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Load large models\n",
    "print(\"Loading large models...\")\n",
    "\n",
    "try:\n",
    "    print(\"  Loading Llama 3.1 8B...\")\n",
    "    llama_model, llama_tokenizer = get_llama_3_8B()\n",
    "    print(f\"    Llama loaded on {llama_model.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"    Error loading Llama: {e}\")\n",
    "    llama_model, llama_tokenizer = None, None\n",
    "\n",
    "print(\"Large models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Load Gemma model\n",
    "print(\"Loading large models...\")\n",
    "\n",
    "try:\n",
    "    print(\"  Loading Gemma 2 9B...\")\n",
    "    gemma_model, gemma_tokenizer = get_gemma_2_9B()\n",
    "    print(f\"    Gemma loaded on {gemma_model.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"    Error loading Gemma: {e}\")\n",
    "    gemma_model, gemma_tokenizer = None, None\n",
    "\n",
    "clear_memory()\n",
    "print(\"Large models loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bab5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Run experiments on large models (Updated with t-test and large model type checking)\n",
    "def run_large_model_experiment_with_type_check(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    model_name: str,\n",
    "    token_type: str,\n",
    "    max_sentences: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    Run experiments on large models with token type checking from small models or large model dict\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RUNNING {model_name.upper()} {token_type.upper()} EXPERIMENT\")\n",
    "    print(f\"Using {TOKEN_SET_TYPE} token sets\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Get token sets and sentences\n",
    "    if token_type not in token_sets:\n",
    "        print(f\"Error: No token sets for {token_type}\")\n",
    "        return None, None\n",
    "\n",
    "    current_token_sets = token_sets[token_type]\n",
    "    sentences = SENTENCES_OF_TYPE[token_type]\n",
    "    test_sentences = random.sample(sentences, min(max_sentences, len(sentences)))\n",
    "\n",
    "    # Load token type mappings for type checking\n",
    "    print(\"Loading token type mappings for type checking...\")\n",
    "    try:\n",
    "        pythia_token_id_to_type, pythia_token_str_to_type = load_type_dicts('pythia')\n",
    "        print(f\"  Loaded Pythia token types: {len(pythia_token_str_to_type)} tokens\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading Pythia token types: {e}\")\n",
    "        pythia_token_str_to_type = {}\n",
    "\n",
    "    try:\n",
    "        gpt2_token_id_to_type, gpt2_token_str_to_type = load_type_dicts('gpt2')\n",
    "        print(f\"  Loaded GPT-2 token types: {len(gpt2_token_str_to_type)} tokens\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading GPT-2 token types: {e}\")\n",
    "        gpt2_token_str_to_type = {}\n",
    "\n",
    "    print(f\"\\nToken set sizes:\")\n",
    "    for category, tokens in current_token_sets.items():\n",
    "        print(f\"  {category}: {len(tokens)}\")\n",
    "    print(f\"Test sentences: {len(test_sentences)}\")\n",
    "\n",
    "    use_large_model_type = model_name.lower() in ['llama', 'gemma']\n",
    "\n",
    "    # Run experiments\n",
    "    results = {}\n",
    "    categories = ['target', 'high_interference', 'medium_interference', 'random']\n",
    "\n",
    "    for category in categories:\n",
    "        if not current_token_sets[category]:\n",
    "            print(f\"\\nSkipping {category} - no tokens available\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTesting {category} tokens...\")\n",
    "        category_results = []\n",
    "\n",
    "        for sent_idx, sentence in enumerate(tqdm(test_sentences, desc=f\"{category}\")):\n",
    "            if sent_idx % 20 == 19:\n",
    "                clear_memory()\n",
    "            try:\n",
    "                experiment_result = run_injection_experiment_large_model_with_type_check(\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    sentence=sentence,\n",
    "                    injection_tokens=current_token_sets[category],\n",
    "                    target_type=token_type,\n",
    "                    pythia_token_str_to_type=pythia_token_str_to_type,\n",
    "                    gpt2_token_str_to_type=gpt2_token_str_to_type,\n",
    "                    n_trials=N_TRIALS,\n",
    "                    top_k=TOP_K,\n",
    "                    injection_size=INJECTION_SIZE,\n",
    "                    use_large_model_type=use_large_model_type,\n",
    "                    large_model_token_type_dict=large_model_token_type_dict\n",
    "                )\n",
    "                category_results.append({\n",
    "                    'sentence_idx': sent_idx,\n",
    "                    'sentence': sentence,\n",
    "                    'results': experiment_result\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in sentence {sent_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        results[category] = category_results\n",
    "\n",
    "    # Analyze results\n",
    "    analysis = analyze_large_model_results(results, model_name, token_type)\n",
    "    print_large_model_summary(analysis, model_name, token_type)\n",
    "\n",
    "    return results, analysis\n",
    "\n",
    "def analyze_large_model_results(results: Dict, model_name: str, token_type: str) -> Dict:\n",
    "    \"\"\"Analyze large model experiment results\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name.upper()} {token_type.upper()} ANALYSIS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    analysis = {}\n",
    "    categories = ['target', 'high_interference', 'medium_interference', 'random']\n",
    "    \n",
    "    for category in categories:\n",
    "        if category not in results or not results[category]:\n",
    "            continue\n",
    "        \n",
    "        # Collect trial results\n",
    "        count_increases = []\n",
    "        prob_increases = []\n",
    "        total_trials = 0\n",
    "        \n",
    "        for sentence_result in results[category]:\n",
    "            for trial_data in sentence_result['results']['detailed_metrics']:\n",
    "                count_increases.append(1 if trial_data['count_increase'] else 0)\n",
    "                prob_increases.append(1 if trial_data['prob_increase'] else 0)\n",
    "                total_trials += 1\n",
    "        \n",
    "        if total_trials == 0:\n",
    "            continue\n",
    "        \n",
    "        count_success_rate = sum(count_increases) / total_trials\n",
    "        prob_success_rate = sum(prob_increases) / total_trials\n",
    "        \n",
    "        analysis[category] = {\n",
    "            'total_trials': total_trials,\n",
    "            'count_increases': sum(count_increases),\n",
    "            'prob_increases': sum(prob_increases),\n",
    "            'count_success_rate': count_success_rate,\n",
    "            'prob_success_rate': prob_success_rate,\n",
    "            'count_increases_raw': count_increases,\n",
    "            'prob_increases_raw': prob_increases\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(f\"  Total trials: {total_trials}\")\n",
    "        print(f\"  Count increases: {sum(count_increases)} ({count_success_rate:.3f})\")\n",
    "        print(f\"  Prob increases: {sum(prob_increases)} ({prob_success_rate:.3f})\")\n",
    "    \n",
    "    # Statistical comparisons using t-test\n",
    "    if 'random' in analysis:\n",
    "        random_data = analysis['random']\n",
    "        print(f\"\\nSTATISTICAL COMPARISONS (t-test):\")\n",
    "        \n",
    "        for category in ['target', 'high_interference', 'medium_interference']:\n",
    "            if category not in analysis:\n",
    "                continue\n",
    "            \n",
    "            category_data = analysis[category]\n",
    "            \n",
    "            # Count increase comparison using t-test\n",
    "            try:\n",
    "                count_stat, count_pval = ttest_ind(\n",
    "                    category_data['count_increases_raw'],\n",
    "                    random_data['count_increases_raw'],\n",
    "                    equal_var=False  # Welch's t-test (unequal variances)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in count t-test for {category}: {e}\")\n",
    "                count_stat, count_pval = 0.0, 1.0\n",
    "            \n",
    "            # Probability increase comparison using t-test\n",
    "            try:\n",
    "                prob_stat, prob_pval = ttest_ind(\n",
    "                    category_data['prob_increases_raw'],\n",
    "                    random_data['prob_increases_raw'],\n",
    "                    equal_var=False  # Welch's t-test (unequal variances)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"    Error in prob t-test for {category}: {e}\")\n",
    "                prob_stat, prob_pval = 0.0, 1.0\n",
    "            \n",
    "            analysis[f'{category}_vs_random'] = {\n",
    "                'count_diff': category_data['count_success_rate'] - random_data['count_success_rate'],\n",
    "                'count_pvalue': count_pval,\n",
    "                'count_significant': count_pval < 0.05,\n",
    "                'count_tstat': count_stat,\n",
    "                'prob_diff': category_data['prob_success_rate'] - random_data['prob_success_rate'],\n",
    "                'prob_pvalue': prob_pval,\n",
    "                'prob_significant': prob_pval < 0.05,\n",
    "                'prob_tstat': prob_stat\n",
    "            }\n",
    "            \n",
    "            print(f\"\\n{category} vs random:\")\n",
    "            print(f\"  Count diff: {analysis[f'{category}_vs_random']['count_diff']:+.3f} (t={count_stat:.3f}, p={count_pval:.4f})\")\n",
    "            print(f\"  Prob diff: {analysis[f'{category}_vs_random']['prob_diff']:+.3f} (t={prob_stat:.3f}, p={prob_pval:.4f})\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def print_large_model_summary(analysis: Dict, model_name: str, token_type: str):\n",
    "    \"\"\"Print summary of large model results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name.upper()} {token_type.upper()} SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    categories = ['target', 'high_interference', 'medium_interference', 'random']\n",
    "    \n",
    "    print(f\"{'Category':<20} {'Count Rate':<12} {'Prob Rate':<12} {'vs Random':<15}\")\n",
    "    print(f\"{'-'*20} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "    \n",
    "    for category in categories:\n",
    "        if category not in analysis:\n",
    "            continue\n",
    "        \n",
    "        count_rate = analysis[category]['count_success_rate']\n",
    "        prob_rate = analysis[category]['prob_success_rate']\n",
    "        \n",
    "        vs_random_str = \"\"\n",
    "        if f'{category}_vs_random' in analysis:\n",
    "            vs_random = analysis[f'{category}_vs_random']\n",
    "            count_sig = \"**\" if vs_random['count_significant'] else \"\"\n",
    "            prob_sig = \"**\" if vs_random['prob_significant'] else \"\"\n",
    "            vs_random_str = f\"{vs_random['count_diff']:+.3f}{count_sig}/{vs_random['prob_diff']:+.3f}{prob_sig}\"\n",
    "        elif category == 'random':\n",
    "            vs_random_str = \"baseline\"\n",
    "        \n",
    "        print(f\"{category:<20} {count_rate:<12.3f} {prob_rate:<12.3f} {vs_random_str:<15}\")\n",
    "    \n",
    "    print(\"\\n** = statistically significant (p < 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39484565",
   "metadata": {},
   "source": [
    "## Llama Exp using Large Model Token Type Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8: Run experiments on LlaMa\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_for_json(obj):\n",
    "    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_for_json(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_for_json(item) for item in obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def safe_json_dump(data, file_path):\n",
    "    \"\"\"Safely dump data to JSON file with type conversion\"\"\"\n",
    "    converted_data = convert_for_json(data)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = Path(\"./results/llama_gemma_prompt_injection\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create timestamp for this experiment run\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_id = f\"{TOKEN_SET_TYPE}_{N_SELECTED_ACT_TOKENS}tk_{PYTHIA_HIGH_INTERFERENCE_THRESHOLD}ph_{GPT2_HIGH_INTERFERENCE_THRESHOLD}gh_{timestamp}\"\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "experiment_results = {}\n",
    "\n",
    "models = []\n",
    "if llama_model and llama_tokenizer:\n",
    "    models.append(('llama', llama_model, llama_tokenizer))\n",
    "# if gemma_model and gemma_tokenizer:\n",
    "#     models.append(('gemma', gemma_model, gemma_tokenizer))\n",
    "\n",
    "test_token_types = [\n",
    "    'location', 'person', 'emotion', 'color', 'animal', 'number', 'science', 'time'\n",
    "]\n",
    "\n",
    "# Save experiment metadata\n",
    "experiment_metadata = {\n",
    "    'experiment_id': experiment_id,\n",
    "    'timestamp': timestamp,\n",
    "    'parameters': {\n",
    "        'TOKEN_SET_TYPE': TOKEN_SET_TYPE,\n",
    "        'N_SELECTED_ACT_TOKENS': N_SELECTED_ACT_TOKENS,\n",
    "        'PYTHIA_HIGH_INTERFERENCE_THRESHOLD': float(PYTHIA_HIGH_INTERFERENCE_THRESHOLD),\n",
    "        'GPT2_HIGH_INTERFERENCE_THRESHOLD': float(GPT2_HIGH_INTERFERENCE_THRESHOLD),\n",
    "        'BASELINE_INTERFERENCE_THRESHOLD': float(BASELINE_INTERFERENCE_THRESHOLD),\n",
    "        'N_TRIALS': int(N_TRIALS),\n",
    "        'TOP_K': int(TOP_K),\n",
    "        'INJECTION_SIZE': int(INJECTION_SIZE),\n",
    "        'RANDOM_SEED': int(RANDOM_SEED)\n",
    "    },\n",
    "    'models': [name for name, _, _ in models],\n",
    "    'token_types': test_token_types,\n",
    "    'max_sentences_per_type': 50\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = RESULTS_DIR / f\"{experiment_id}_metadata.json\"\n",
    "safe_json_dump(experiment_metadata, metadata_file)\n",
    "print(f\"Saved metadata to: {metadata_file}\")\n",
    "\n",
    "for model_name, model, tokenizer in models:\n",
    "    experiment_results[model_name] = {}\n",
    "    \n",
    "    for token_type in test_token_types:\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"STARTING {model_name.upper()} {token_type.upper()} EXPERIMENT\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        try:\n",
    "            results, analysis = run_large_model_experiment_with_type_check(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                model_name=model_name,\n",
    "                token_type=token_type,\n",
    "                max_sentences=N_TEST_SENTENCES\n",
    "            )\n",
    "            \n",
    "            experiment_results[model_name][token_type] = {\n",
    "                'results': results,\n",
    "                'analysis': analysis\n",
    "            }\n",
    "            \n",
    "            # Save individual result file for each model-token_type combination\n",
    "            individual_result = {\n",
    "                'experiment_id': experiment_id,\n",
    "                'model_name': model_name,\n",
    "                'token_type': token_type,\n",
    "                'timestamp': timestamp,\n",
    "                'parameters': experiment_metadata['parameters'],\n",
    "                'token_sets': token_sets[token_type],\n",
    "                'results': results,\n",
    "                'analysis': analysis\n",
    "            }\n",
    "            \n",
    "            individual_file = RESULTS_DIR / f\"{experiment_id}_{model_name}_{token_type}.json\"\n",
    "            safe_json_dump(individual_result, individual_file)\n",
    "            print(f\"Saved {model_name} {token_type} results to: {individual_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {model_name} {token_type} experiment: {e}\")\n",
    "            \n",
    "            # Save error info\n",
    "            error_result = {\n",
    "                'experiment_id': experiment_id,\n",
    "                'model_name': model_name,\n",
    "                'token_type': token_type,\n",
    "                'timestamp': timestamp,\n",
    "                'error': str(e),\n",
    "                'parameters': experiment_metadata['parameters']\n",
    "            }\n",
    "            \n",
    "            error_file = RESULTS_DIR / f\"{experiment_id}_{model_name}_{token_type}_ERROR.json\"\n",
    "            safe_json_dump(error_result, error_file)\n",
    "            print(f\"Saved error info to: {error_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Clear memory between experiments\n",
    "        clear_memory()\n",
    "\n",
    "# Save complete experiment results\n",
    "complete_results = {\n",
    "    'experiment_id': experiment_id,\n",
    "    'timestamp': timestamp,\n",
    "    'metadata': experiment_metadata,\n",
    "    'results': experiment_results\n",
    "}\n",
    "\n",
    "complete_file = RESULTS_DIR / f\"{experiment_id}_complete.json\"\n",
    "safe_json_dump(complete_results, complete_file)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL LARGE MODEL EXPERIMENTS COMPLETE\")\n",
    "print(f\"Saved complete results to: {complete_file}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab23ed2",
   "metadata": {},
   "source": [
    "## Gemma Exp using Large Model Token Type Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f67de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8: Run experiments on LlaMa\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_for_json(obj):\n",
    "    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_for_json(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_for_json(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(convert_for_json(item) for item in obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def safe_json_dump(data, file_path):\n",
    "    \"\"\"Safely dump data to JSON file with type conversion\"\"\"\n",
    "    converted_data = convert_for_json(data)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Create results directory\n",
    "RESULTS_DIR = Path(\"./results/llama_gemma_prompt_injection\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create timestamp for this experiment run\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_id = f\"{TOKEN_SET_TYPE}_{N_SELECTED_ACT_TOKENS}tk_{PYTHIA_HIGH_INTERFERENCE_THRESHOLD}ph_{GPT2_HIGH_INTERFERENCE_THRESHOLD}gh_{timestamp}\"\n",
    "\n",
    "print(f\"Experiment ID: {experiment_id}\")\n",
    "print(f\"Results will be saved to: {RESULTS_DIR}\")\n",
    "\n",
    "experiment_results = {}\n",
    "\n",
    "models = []\n",
    "# if llama_model and llama_tokenizer:\n",
    "#     models.append(('llama', llama_model, llama_tokenizer))\n",
    "if gemma_model and gemma_tokenizer:\n",
    "    models.append(('gemma', gemma_model, gemma_tokenizer))\n",
    "\n",
    "test_token_types = [\n",
    "    'location', 'person', 'emotion', 'color', 'animal', 'number', 'science', 'time'\n",
    "]\n",
    "\n",
    "# Save experiment metadata\n",
    "experiment_metadata = {\n",
    "    'experiment_id': experiment_id,\n",
    "    'timestamp': timestamp,\n",
    "    'parameters': {\n",
    "        'TOKEN_SET_TYPE': TOKEN_SET_TYPE,\n",
    "        'N_SELECTED_ACT_TOKENS': N_SELECTED_ACT_TOKENS,\n",
    "        'PYTHIA_HIGH_INTERFERENCE_THRESHOLD': float(PYTHIA_HIGH_INTERFERENCE_THRESHOLD),\n",
    "        'GPT2_HIGH_INTERFERENCE_THRESHOLD': float(GPT2_HIGH_INTERFERENCE_THRESHOLD),\n",
    "        'BASELINE_INTERFERENCE_THRESHOLD': float(BASELINE_INTERFERENCE_THRESHOLD),\n",
    "        'N_TRIALS': int(N_TRIALS),\n",
    "        'TOP_K': int(TOP_K),\n",
    "        'INJECTION_SIZE': int(INJECTION_SIZE),\n",
    "        'RANDOM_SEED': int(RANDOM_SEED)\n",
    "    },\n",
    "    'models': [name for name, _, _ in models],\n",
    "    'token_types': test_token_types,\n",
    "    'max_sentences_per_type': 50\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = RESULTS_DIR / f\"{experiment_id}_metadata.json\"\n",
    "safe_json_dump(experiment_metadata, metadata_file)\n",
    "print(f\"Saved metadata to: {metadata_file}\")\n",
    "\n",
    "for model_name, model, tokenizer in models:\n",
    "    experiment_results[model_name] = {}\n",
    "    \n",
    "    for token_type in test_token_types:\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"STARTING {model_name.upper()} {token_type.upper()} EXPERIMENT\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        try:\n",
    "            results, analysis = run_large_model_experiment_with_type_check(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                model_name=model_name,\n",
    "                token_type=token_type,\n",
    "                max_sentences=N_TEST_SENTENCES\n",
    "            )\n",
    "            \n",
    "            experiment_results[model_name][token_type] = {\n",
    "                'results': results,\n",
    "                'analysis': analysis\n",
    "            }\n",
    "            \n",
    "            # Save individual result file for each model-token_type combination\n",
    "            individual_result = {\n",
    "                'experiment_id': experiment_id,\n",
    "                'model_name': model_name,\n",
    "                'token_type': token_type,\n",
    "                'timestamp': timestamp,\n",
    "                'parameters': experiment_metadata['parameters'],\n",
    "                'token_sets': token_sets[token_type],\n",
    "                'results': results,\n",
    "                'analysis': analysis\n",
    "            }\n",
    "            \n",
    "            individual_file = RESULTS_DIR / f\"{experiment_id}_{model_name}_{token_type}.json\"\n",
    "            safe_json_dump(individual_result, individual_file)\n",
    "            print(f\"Saved {model_name} {token_type} results to: {individual_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {model_name} {token_type} experiment: {e}\")\n",
    "            \n",
    "            # Save error info\n",
    "            error_result = {\n",
    "                'experiment_id': experiment_id,\n",
    "                'model_name': model_name,\n",
    "                'token_type': token_type,\n",
    "                'timestamp': timestamp,\n",
    "                'error': str(e),\n",
    "                'parameters': experiment_metadata['parameters']\n",
    "            }\n",
    "            \n",
    "            error_file = RESULTS_DIR / f\"{experiment_id}_{model_name}_{token_type}_ERROR.json\"\n",
    "            safe_json_dump(error_result, error_file)\n",
    "            print(f\"Saved error info to: {error_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Clear memory between experiments\n",
    "        clear_memory()\n",
    "\n",
    "# Save complete experiment results\n",
    "complete_results = {\n",
    "    'experiment_id': experiment_id,\n",
    "    'timestamp': timestamp,\n",
    "    'metadata': experiment_metadata,\n",
    "    'results': experiment_results\n",
    "}\n",
    "\n",
    "complete_file = RESULTS_DIR / f\"{experiment_id}_complete.json\"\n",
    "safe_json_dump(complete_results, complete_file)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL LARGE MODEL EXPERIMENTS COMPLETE\")\n",
    "print(f\"Saved complete results to: {complete_file}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
