{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e1f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Import packages (Updated)\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "\n",
    "# Import model utilities and sentences\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath('__file__')))\n",
    "from utils.utils_model import get_hooked_pythia_70m, get_hooked_gpt2_small\n",
    "from utils.utils_data import load_type_dicts, get_token_id_type, get_token_str_type\n",
    "from corpus.type_sentences import SENTENCES_OF_TYPE\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311838d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Define utility functions\n",
    "def inject_tokens(sentence: str, tokens: List[str]) -> str:\n",
    "    \"\"\"Inject tokens at the beginning of sentence with parentheses\"\"\"\n",
    "    return f\"({''.join(tokens)}){sentence}\"\n",
    "\n",
    "def get_next_token_probs(model, text: str, top_k: int = 10) -> List[Tuple[int, str, float]]:\n",
    "    \"\"\"Get top-k next token predictions with probabilities\"\"\"\n",
    "    tokens = model.to_tokens(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(tokens)[0, -1, :]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, top_k)\n",
    "    return [\n",
    "        (idx.item(), model.to_string(idx.item()), prob.item())\n",
    "        for idx, prob in zip(top_k_indices, top_k_probs)\n",
    "    ]\n",
    "\n",
    "def get_weighted_cosine_similarity(embeddings: torch.Tensor, \n",
    "                                 token_ids: List[int], \n",
    "                                 token_probs: List[float], \n",
    "                                 target_ids: List[int]) -> float:\n",
    "    \"\"\"Calculate weighted cosine similarity between predicted and target tokens\"\"\"\n",
    "    if not token_ids or not target_ids:\n",
    "        return 0.0\n",
    "    \n",
    "    # Get embeddings\n",
    "    output_embeddings = embeddings[token_ids]\n",
    "    target_embeddings = embeddings[target_ids]\n",
    "    \n",
    "    # Normalize\n",
    "    output_embeddings = F.normalize(output_embeddings, p=2, dim=1)\n",
    "    target_embeddings = F.normalize(target_embeddings, p=2, dim=1)\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = torch.matmul(output_embeddings, target_embeddings.T)\n",
    "    max_similarities = torch.max(similarity_matrix, dim=1)[0]\n",
    "    \n",
    "    # Apply weights\n",
    "    token_probs_tensor = torch.tensor(token_probs, device=max_similarities.device)\n",
    "    weighted_similarities = max_similarities * token_probs_tensor\n",
    "    \n",
    "    return torch.sum(weighted_similarities).item()\n",
    "\n",
    "def get_weighted_overlap(token_ids: List[int], target_ids: List[int], token_probs: List[float]) -> float:\n",
    "    \"\"\"Calculate weighted overlap between predicted and target tokens\"\"\"\n",
    "    overlap = 0.0\n",
    "    for i, token_id in enumerate(token_ids):\n",
    "        if token_id in target_ids:\n",
    "            overlap += token_probs[i]\n",
    "    return overlap\n",
    "\n",
    "def get_best_rank(token_ids: List[int], target_ids: List[int]) -> Optional[int]:\n",
    "    \"\"\"Get the best rank of target tokens in predicted tokens\"\"\"\n",
    "    for i, token_id in enumerate(token_ids):\n",
    "        if token_id in target_ids:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4080a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "  Pythia high interference threshold: 0.5\n",
      "  GPT-2 high interference threshold: 0.3\n",
      "  Baseline interference threshold: 0.2\n",
      "  Number of trials per experiment: 100\n",
      "  Top-k predictions: 30\n"
     ]
    }
   ],
   "source": [
    "# 3: Define parameters\n",
    "# Interference thresholds\n",
    "PYTHIA_HIGH_INTERFERENCE_THRESHOLD = 0.5\n",
    "GPT2_HIGH_INTERFERENCE_THRESHOLD = 0.3\n",
    "BASELINE_INTERFERENCE_THRESHOLD = 0.2\n",
    "\n",
    "# Experiment parameters\n",
    "N_SELECTED_TOKENS = 10\n",
    "N_TRIALS = 100\n",
    "TOP_K = 30\n",
    "RANDOM_SEED = 43\n",
    "N_INJECTION = 30\n",
    "\n",
    "# File paths\n",
    "MED_DATA_DIR = Path(\"./med_data\")\n",
    "\n",
    "print(f\"Parameters:\")\n",
    "print(f\"  Pythia high interference threshold: {PYTHIA_HIGH_INTERFERENCE_THRESHOLD}\")\n",
    "print(f\"  GPT-2 high interference threshold: {GPT2_HIGH_INTERFERENCE_THRESHOLD}\")\n",
    "print(f\"  Baseline interference threshold: {BASELINE_INTERFERENCE_THRESHOLD}\")\n",
    "print(f\"  Number of trials per experiment: {N_TRIALS}\")\n",
    "print(f\"  Top-k predictions: {TOP_K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4: Load models\n",
    "print(\"Loading models...\")\n",
    "\n",
    "# Load Pythia\n",
    "print(\"  Loading Pythia-70M...\")\n",
    "pythia = get_hooked_pythia_70m(device)\n",
    "pythia_embedding = pythia.embed.W_E.detach()\n",
    "print(f\"    Pythia loaded on {next(pythia.parameters()).device}\")\n",
    "\n",
    "# Load GPT-2\n",
    "print(\"  Loading GPT-2 Small...\")\n",
    "gpt2 = get_hooked_gpt2_small(device)\n",
    "gpt2_embedding = gpt2.embed.W_E.detach()\n",
    "print(f\"    GPT-2 loaded on {next(gpt2.parameters()).device}\")\n",
    "\n",
    "clear_memory()\n",
    "print(\"Models loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea30de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Define function to get interference token sets (Updated to remove intersection)\n",
    "def get_interference_token_sets(\n",
    "    model_name: str,\n",
    "    token_type: str,\n",
    "    high_threshold: float,\n",
    "    baseline_threshold: float,\n",
    "    med_data_dir: Path = MED_DATA_DIR\n",
    ") -> Dict[str, Set[str]]:\n",
    "    \"\"\"\n",
    "    Get interference token sets for a given model and token type\n",
    "    \n",
    "    Args:\n",
    "        model_name: 'pythia' or 'gpt2'\n",
    "        token_type: Target token type (e.g., 'location', 'person', 'emotion')\n",
    "        high_threshold: Threshold for high interference tokens\n",
    "        baseline_threshold: Threshold for baseline interference\n",
    "        med_data_dir: Directory containing interference data files\n",
    "    \n",
    "    Returns:\n",
    "        Dict with token sets: 'target', 'high_interference', 'medium_interference', 'random'\n",
    "    \"\"\"\n",
    "    print(f\"\\nGetting interference token sets for {model_name} - {token_type}\")\n",
    "    \n",
    "    # Load interference data\n",
    "    interference_file = med_data_dir / f\"{model_name}_{token_type}_tokens_r0.8_i0.2_s0.3.json\"\n",
    "    \n",
    "    if not interference_file.exists():\n",
    "        raise FileNotFoundError(f\"Interference file not found: {interference_file}\")\n",
    "    \n",
    "    print(f\"  Loading: {interference_file}\")\n",
    "    with open(interference_file, 'r', encoding='utf-8') as f:\n",
    "        interference_data = json.load(f)\n",
    "    \n",
    "    # Load token type dictionaries using utils_data\n",
    "    print(f\"  Loading token type dictionaries...\")\n",
    "    token_id_to_type_dict, token_str_to_type_dict = load_type_dicts(model_name)\n",
    "    print(f\"    Loaded {len(token_str_to_type_dict)} token type mappings\")\n",
    "    \n",
    "    # Get model for token operations\n",
    "    if model_name == 'pythia':\n",
    "        model = pythia\n",
    "    elif model_name == 'gpt2':\n",
    "        model = gpt2\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    # Initialize token sets\n",
    "    target_tokens = set()\n",
    "    high_interference_tokens = set()\n",
    "    medium_interference_tokens = set()\n",
    "    \n",
    "    # Get target tokens from vocabulary (all tokens of target type)\n",
    "    print(f\"  Getting target tokens from vocabulary...\")\n",
    "    for token_str, token_type_label in token_str_to_type_dict.items():\n",
    "        if token_type_label == token_type:\n",
    "            target_tokens.add(token_str)\n",
    "    \n",
    "    print(f\"    Found {len(target_tokens)} target tokens in vocabulary\")\n",
    "    \n",
    "    # Extract interference tokens from target features and interference features\n",
    "    print(f\"  Processing interference data...\")\n",
    "    \n",
    "    for layer_type, layers in interference_data.items():\n",
    "        if layer_type == 'summary':\n",
    "            continue\n",
    "        \n",
    "        if not isinstance(layers, dict):\n",
    "            continue\n",
    "            \n",
    "        for layer_idx, layer_data in layers.items():\n",
    "            if not isinstance(layer_data, dict):\n",
    "                continue\n",
    "            \n",
    "            # Process target features - add non-target-type tokens to interference sets\n",
    "            # target_features = layer_data.get('target_features', [])\n",
    "            # for feature in target_features:\n",
    "            #     high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                \n",
    "            #     # Check each high activation token's type\n",
    "            #     for token_str in high_activation_tokens:\n",
    "            #         # Use get_token_str_type from utils_data\n",
    "            #         token_type_label = get_token_str_type(\n",
    "            #             model_name, token_str, token_str_to_type_dict\n",
    "            #         )\n",
    "                    \n",
    "            #         # If token is not of target type, it's an interference token\n",
    "            #         if token_type_label != token_type:\n",
    "            #             # Since these come from target features, they are high interference\n",
    "            #             high_interference_tokens.add(token_str)\n",
    "            \n",
    "            # Process interference features\n",
    "            interference_features = layer_data.get('interference_features', [])\n",
    "            for feature in interference_features:\n",
    "                # Check maximum interference value for this feature\n",
    "                interferences = feature.get('interferences', [])\n",
    "                if not interferences:\n",
    "                    continue\n",
    "                \n",
    "                max_interference_value = max(\n",
    "                    interference['interference_value'] \n",
    "                    for interference in interferences\n",
    "                )\n",
    "                \n",
    "                high_activation_tokens = feature.get('high_activation_tokens', [])\n",
    "                \n",
    "                # Categorize based on interference value\n",
    "                if max_interference_value >= high_threshold:\n",
    "                    high_interference_tokens.update(high_activation_tokens[:N_SELECTED_TOKENS])\n",
    "                elif max_interference_value >= baseline_threshold:\n",
    "                    medium_interference_tokens.update(high_activation_tokens[:N_SELECTED_TOKENS])\n",
    "    \n",
    "    # Get all vocabulary tokens for random set\n",
    "    print(f\"  Getting random tokens from vocabulary...\")\n",
    "    vocab_size = model.cfg.d_vocab\n",
    "    all_tokens = set()\n",
    "    for token_id in range(vocab_size):\n",
    "        try:\n",
    "            token_str = model.to_string(token_id)\n",
    "            # Only add if it's a valid string token\n",
    "            if token_str and token_str.strip():\n",
    "                all_tokens.add(token_str)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Calculate random tokens before removing intersections\n",
    "    used_tokens = target_tokens | high_interference_tokens | medium_interference_tokens\n",
    "    random_tokens = all_tokens - used_tokens\n",
    "    \n",
    "    print(f\"  Initial token set sizes (before intersection removal):\")\n",
    "    print(f\"    target: {len(target_tokens)}\")\n",
    "    print(f\"    high_interference: {len(high_interference_tokens)}\")\n",
    "    print(f\"    medium_interference: {len(medium_interference_tokens)}\")\n",
    "    print(f\"    random: {len(random_tokens)}\")\n",
    "    \n",
    "    # Remove intersection between high and medium interference tokens\n",
    "    print(f\"  Removing intersections between high and medium interference...\")\n",
    "    intersection = high_interference_tokens & medium_interference_tokens\n",
    "    print(f\"    Found {len(intersection)} tokens in intersection\")\n",
    "    \n",
    "    if intersection:\n",
    "        # Show sample intersection tokens\n",
    "        sample_intersection = list(intersection)[:5]\n",
    "        print(f\"    Sample intersection tokens: {sample_intersection}\")\n",
    "        \n",
    "        # Remove intersection from medium interference (keep in high interference)\n",
    "        medium_interference_tokens = medium_interference_tokens - intersection\n",
    "        high_interference_tokens = high_interference_tokens - intersection\n",
    "        print(f\"    Removed intersection from medium interference set\")\n",
    "    \n",
    "    # Convert to lists and remove empty/invalid strings\n",
    "    token_sets = {\n",
    "        'target': [t for t in target_tokens if t and t.strip()],\n",
    "        'high_interference': [t for t in high_interference_tokens if t and t.strip()],\n",
    "        'medium_interference': [t for t in medium_interference_tokens if t and t.strip()],\n",
    "        'random': [t for t in random_tokens if t and t.strip()]\n",
    "    }\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"  Final token set sizes (after intersection removal):\")\n",
    "    for set_name, token_list in token_sets.items():\n",
    "        print(f\"    {set_name}: {len(token_list)}\")\n",
    "    \n",
    "    # Verify no intersection remains\n",
    "    high_set = set(token_sets['high_interference'])\n",
    "    medium_set = set(token_sets['medium_interference'])\n",
    "    remaining_intersection = high_set & medium_set\n",
    "    print(f\"  Verification: remaining intersection = {len(remaining_intersection)}\")\n",
    "    \n",
    "    return token_sets\n",
    "\n",
    "def get_target_token_ids(model, token_strs: List[str]) -> List[int]:\n",
    "    \"\"\"Convert token strings to token IDs\"\"\"\n",
    "    token_ids = []\n",
    "    for token_str in token_strs:\n",
    "        try:\n",
    "            token_id = model.to_tokens(token_str, prepend_bos=False)[0]\n",
    "            if len(token_id) == 1:\n",
    "                token_ids.append(token_id.item())\n",
    "            else:\n",
    "                token_ids.append(token_id[0].item())\n",
    "        except:\n",
    "            continue\n",
    "    return list(set(token_ids))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a16a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Get interference token sets for all experiments (Updated with intersection info)\n",
    "# Set random seed\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Define experiment configurations\n",
    "token_types = [\n",
    "    'location', 'person', 'emotion', 'color',\n",
    "    'animal', 'number', 'science', 'time'\n",
    "]\n",
    "\n",
    "EXPERIMENTS = [\n",
    "    {\n",
    "        'token_type': token_type,\n",
    "        'sentences': SENTENCES_OF_TYPE[token_type],\n",
    "        'pythia_threshold': PYTHIA_HIGH_INTERFERENCE_THRESHOLD,\n",
    "        'gpt2_threshold': GPT2_HIGH_INTERFERENCE_THRESHOLD\n",
    "    }\n",
    "    for token_type in token_types\n",
    "]\n",
    "\n",
    "# Get token sets for all experiments\n",
    "print(\"=\"*60)\n",
    "print(\"LOADING INTERFERENCE TOKEN SETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "experiment_token_sets = {}\n",
    "\n",
    "# Pre-load token type mappings using utils_data\n",
    "print(\"Loading token type mappings...\")\n",
    "try:\n",
    "    pythia_token_id_to_type, pythia_token_str_to_type = load_type_dicts('pythia')\n",
    "    print(f\"  Loaded Pythia token types: {len(pythia_token_str_to_type)} tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error loading Pythia token types: {e}\")\n",
    "    pythia_token_str_to_type = {}\n",
    "\n",
    "try:\n",
    "    gpt2_token_id_to_type, gpt2_token_str_to_type = load_type_dicts('gpt2')\n",
    "    print(f\"  Loaded GPT-2 token types: {len(gpt2_token_str_to_type)} tokens\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error loading GPT-2 token types: {e}\")\n",
    "    gpt2_token_str_to_type = {}\n",
    "\n",
    "for exp_config in EXPERIMENTS:\n",
    "    token_type = exp_config['token_type']\n",
    "    \n",
    "    print(f\"\\n--- {token_type.upper()} EXPERIMENT ---\")\n",
    "    \n",
    "    # Get Pythia token sets\n",
    "    try:\n",
    "        pythia_sets = get_interference_token_sets(\n",
    "            model_name='pythia',\n",
    "            token_type=token_type,\n",
    "            high_threshold=exp_config['pythia_threshold'],\n",
    "            baseline_threshold=BASELINE_INTERFERENCE_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Convert target tokens to IDs\n",
    "        pythia_target_ids = get_target_token_ids(pythia, pythia_sets['target'])\n",
    "        \n",
    "        # Verify target tokens are actually of target type using utils_data\n",
    "        target_type_count = 0\n",
    "        for token in pythia_sets['target']:\n",
    "            token_type_result = get_token_str_type('pythia', token, pythia_token_str_to_type)\n",
    "            if token_type_result == token_type:\n",
    "                target_type_count += 1\n",
    "        \n",
    "        print(f\"  Pythia target verification: {target_type_count}/{len(pythia_sets['target'])} are {token_type} type\")\n",
    "        \n",
    "        # Verify no intersection between high and medium interference\n",
    "        pythia_high_set = set(pythia_sets['high_interference'])\n",
    "        pythia_medium_set = set(pythia_sets['medium_interference'])\n",
    "        pythia_intersection = pythia_high_set & pythia_medium_set\n",
    "        print(f\"  Pythia intersection verification: {len(pythia_intersection)} tokens overlap\")\n",
    "        \n",
    "        experiment_token_sets[f'pythia_{token_type}'] = {\n",
    "            'token_sets': pythia_sets,\n",
    "            'target_ids': pythia_target_ids,\n",
    "            'sentences': exp_config['sentences']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading Pythia {token_type}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Get GPT-2 token sets  \n",
    "    try:\n",
    "        gpt2_sets = get_interference_token_sets(\n",
    "            model_name='gpt2',\n",
    "            token_type=token_type,\n",
    "            high_threshold=exp_config['gpt2_threshold'],\n",
    "            baseline_threshold=BASELINE_INTERFERENCE_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Convert target tokens to IDs\n",
    "        gpt2_target_ids = get_target_token_ids(gpt2, gpt2_sets['target'])\n",
    "        \n",
    "        # Verify target tokens are actually of target type using utils_data\n",
    "        target_type_count = 0\n",
    "        for token in gpt2_sets['target']:\n",
    "            token_type_result = get_token_str_type('gpt2', token, gpt2_token_str_to_type)\n",
    "            if token_type_result == token_type:\n",
    "                target_type_count += 1\n",
    "        \n",
    "        print(f\"  GPT-2 target verification: {target_type_count}/{len(gpt2_sets['target'])} are {token_type} type\")\n",
    "        \n",
    "        # Verify no intersection between high and medium interference\n",
    "        gpt2_high_set = set(gpt2_sets['high_interference'])\n",
    "        gpt2_medium_set = set(gpt2_sets['medium_interference'])\n",
    "        gpt2_intersection = gpt2_high_set & gpt2_medium_set\n",
    "        print(f\"  GPT-2 intersection verification: {len(gpt2_intersection)} tokens overlap\")\n",
    "        \n",
    "        experiment_token_sets[f'gpt2_{token_type}'] = {\n",
    "            'token_sets': gpt2_sets,\n",
    "            'target_ids': gpt2_target_ids,\n",
    "            'sentences': exp_config['sentences']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error loading GPT-2 {token_type}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOKEN SETS LOADING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print detailed summary\n",
    "print(f\"\\nDETAILED SUMMARY:\")\n",
    "for exp_name, exp_data in experiment_token_sets.items():\n",
    "    print(f\"\\n{exp_name.upper()}:\")\n",
    "    token_sets = exp_data['token_sets']\n",
    "    target_ids = exp_data['target_ids']\n",
    "    sentences = exp_data['sentences']\n",
    "    \n",
    "    print(f\"  Target tokens: {len(token_sets['target'])} -> {len(target_ids)} IDs\")\n",
    "    print(f\"  High interference: {len(token_sets['high_interference'])}\")\n",
    "    print(f\"  Medium interference: {len(token_sets['medium_interference'])}\")\n",
    "    print(f\"  Random tokens: {len(token_sets['random'])}\")\n",
    "    print(f\"  Test sentences: {len(sentences)}\")\n",
    "    \n",
    "    # Show sample tokens from each category\n",
    "    for category, tokens in token_sets.items():\n",
    "        if tokens and category == 'high_interference':\n",
    "            sample_tokens = tokens  # Show all tokens\n",
    "            print(f\"    {category} samples: {sample_tokens}\")\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d16d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7: Define experiment functions (Fixed)\n",
    "def run_injection_experiment(\n",
    "    model,\n",
    "    sentence: str,\n",
    "    injection_tokens: List[str],\n",
    "    target_ids: List[int],\n",
    "    n_trials: int = N_TRIALS,\n",
    "    top_k: int = TOP_K,\n",
    "    injection_size: int = 10\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run injection experiments with specified tokens\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        sentence: input sentence\n",
    "        injection_tokens: tokens to inject \n",
    "        target_ids: target token ids to measure against\n",
    "        n_trials: number of trials\n",
    "        top_k: number of top predictions to consider\n",
    "        injection_size: number of tokens to inject per trial\n",
    "    \n",
    "    Returns:\n",
    "        dict with success metrics and detailed trial data\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'count_increases': 0,\n",
    "        'prob_increases': 0,\n",
    "        'total_trials': n_trials,\n",
    "        'detailed_metrics': []\n",
    "    }\n",
    "    \n",
    "    # Convert target_ids to set for faster lookup\n",
    "    target_ids_set = set(target_ids) if target_ids else set()\n",
    "    \n",
    "    # Get original predictions\n",
    "    original_top_k = get_next_token_probs(model, sentence, top_k)\n",
    "    original_ids = [t[0] for t in original_top_k] \n",
    "    original_probs = [t[2] for t in original_top_k]\n",
    "    \n",
    "    # Count original target tokens and their total probability\n",
    "    original_count = sum(1 for tid in original_ids if tid in target_ids_set)\n",
    "    original_prob = sum(p for tid, p in zip(original_ids, original_probs) if tid in target_ids_set)\n",
    "    \n",
    "    # Convert to float to avoid tensor issues\n",
    "    original_count = float(original_count)\n",
    "    original_prob = float(original_prob)\n",
    "    \n",
    "    # Run injection trials\n",
    "    for trial in range(n_trials):\n",
    "        try:\n",
    "            # Randomly sample tokens for injection\n",
    "            if len(injection_tokens) >= injection_size:\n",
    "                selected_tokens = random.sample(injection_tokens, injection_size)\n",
    "            else:\n",
    "                # If not enough tokens, sample with replacement\n",
    "                selected_tokens = random.choices(injection_tokens, k=injection_size)\n",
    "            \n",
    "            # Create injected sentence\n",
    "            injected_sentence = inject_tokens(sentence, selected_tokens)\n",
    "            \n",
    "            # Get predictions for injected sentence\n",
    "            injected_top_k = get_next_token_probs(model, injected_sentence, top_k)\n",
    "            injected_ids = [t[0] for t in injected_top_k]\n",
    "            injected_probs = [t[2] for t in injected_top_k]\n",
    "            \n",
    "            # Count injected target tokens and their total probability\n",
    "            injected_count = sum(1 for tid in injected_ids if tid in target_ids_set)\n",
    "            injected_prob = sum(p for tid, p in zip(injected_ids, injected_probs) if tid in target_ids_set)\n",
    "            \n",
    "            # Convert to float to avoid tensor issues\n",
    "            injected_count = float(injected_count)\n",
    "            injected_prob = float(injected_prob)\n",
    "            \n",
    "            # Calculate additional metrics\n",
    "            try:\n",
    "                original_sim = get_weighted_cosine_similarity(\n",
    "                    model.embed.W_E.detach(),\n",
    "                    torch.tensor(original_ids, dtype=torch.long),\n",
    "                    torch.tensor(original_probs, dtype=torch.float32),\n",
    "                    torch.tensor(target_ids, dtype=torch.long)\n",
    "                )\n",
    "                \n",
    "                injected_sim = get_weighted_cosine_similarity(\n",
    "                    model.embed.W_E.detach(),\n",
    "                    torch.tensor(injected_ids, dtype=torch.long),\n",
    "                    torch.tensor(injected_probs, dtype=torch.float32),\n",
    "                    torch.tensor(target_ids, dtype=torch.long)\n",
    "                )\n",
    "                \n",
    "                sim_change = float(injected_sim - original_sim)\n",
    "                \n",
    "            except Exception as e:\n",
    "                sim_change = 0.0\n",
    "            \n",
    "            try:\n",
    "                original_overlap = get_weighted_overlap(original_ids, target_ids, original_probs)\n",
    "                injected_overlap = get_weighted_overlap(injected_ids, target_ids, injected_probs)\n",
    "                overlap_change = float(injected_overlap - original_overlap)\n",
    "            except Exception as e:\n",
    "                overlap_change = 0.0\n",
    "            \n",
    "            try:\n",
    "                original_rank = get_best_rank(original_ids, target_ids)\n",
    "                injected_rank = get_best_rank(injected_ids, target_ids)\n",
    "                rank_change = None\n",
    "                if original_rank is not None and injected_rank is not None:\n",
    "                    rank_change = float(original_rank - injected_rank)  # Positive = improvement (lower rank)\n",
    "                elif original_rank is None and injected_rank is not None:\n",
    "                    rank_change = float(top_k - injected_rank)  # Improvement from no rank to some rank\n",
    "            except Exception as e:\n",
    "                rank_change = None\n",
    "            \n",
    "            # Record trial data - ensure all values are Python primitives\n",
    "            trial_data = {\n",
    "                'trial': trial,\n",
    "                'selected_tokens': selected_tokens,\n",
    "                'original_count': original_count,\n",
    "                'injected_count': injected_count,\n",
    "                'original_prob': original_prob,\n",
    "                'injected_prob': injected_prob,\n",
    "                'count_increase': bool(injected_count > original_count),  # Explicit bool conversion\n",
    "                'prob_increase': bool(injected_prob > original_prob),     # Explicit bool conversion\n",
    "                'similarity_change': sim_change,\n",
    "                'overlap_change': overlap_change,\n",
    "                'rank_change': rank_change,\n",
    "                'injected_sentence': injected_sentence,\n",
    "                'original_predictions': original_top_k,\n",
    "                'injected_predictions': injected_top_k\n",
    "            }\n",
    "            results['detailed_metrics'].append(trial_data)\n",
    "            \n",
    "            # Update success counters\n",
    "            if injected_count > original_count:\n",
    "                results['count_increases'] += 1\n",
    "            if injected_prob > original_prob:\n",
    "                results['prob_increases'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error in trial {trial}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def run_sentence_experiments(\n",
    "    model,\n",
    "    model_name: str,\n",
    "    token_sets: Dict[str, List[str]],\n",
    "    target_ids: List[int],\n",
    "    sentences: List[str],\n",
    "    max_sentences: int = 50\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run experiments for multiple sentences with different token sets\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        model_name: name of the model for logging\n",
    "        token_sets: dict of token categories and their tokens\n",
    "        target_ids: target token IDs\n",
    "        sentences: list of test sentences\n",
    "        max_sentences: maximum number of sentences to test\n",
    "    \n",
    "    Returns:\n",
    "        dict with results for each token category\n",
    "    \"\"\"\n",
    "    # Limit number of sentences\n",
    "    test_sentences = sentences[:max_sentences]\n",
    "    \n",
    "    results = {\n",
    "        'target': [],\n",
    "        'high_interference': [],\n",
    "        'medium_interference': [],\n",
    "        'random': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nRunning experiments for {model_name}\")\n",
    "    print(f\"Testing {len(test_sentences)} sentences with {N_TRIALS} trials each\")\n",
    "    print(f\"Token set sizes:\")\n",
    "    for category, tokens in token_sets.items():\n",
    "        if tokens:  # Only show non-empty sets\n",
    "            print(f\"  {category}: {len(tokens)}\")\n",
    "    \n",
    "    # Run experiments for each sentence\n",
    "    for sent_idx, sentence in enumerate(tqdm(test_sentences, desc=f\"{model_name} sentences\")):\n",
    "        clear_memory()  # Clear memory between sentences\n",
    "        \n",
    "        # Test each token category\n",
    "        for category in ['target', 'high_interference', 'medium_interference', 'random']:\n",
    "            if category not in token_sets or not token_sets[category]:\n",
    "                if sent_idx == 0:  # Only warn once\n",
    "                    print(f\"    Warning: No {category} tokens available, skipping\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                experiment_result = run_injection_experiment(\n",
    "                    model=model,\n",
    "                    sentence=sentence,\n",
    "                    injection_tokens=token_sets[category],\n",
    "                    target_ids=target_ids,\n",
    "                    n_trials=N_TRIALS,\n",
    "                    top_k=TOP_K,\n",
    "                    injection_size=N_INJECTION\n",
    "                )\n",
    "                \n",
    "                results[category].append({\n",
    "                    'sentence_idx': sent_idx,\n",
    "                    'sentence': sentence,\n",
    "                    'results': experiment_result\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error in {category} experiment for sentence {sent_idx}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfadc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8: Statistical analysis functions\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "def analyze_experiment_results(\n",
    "    results: Dict,\n",
    "    model_name: str,\n",
    "    token_type: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze experiment results with statistical tests\n",
    "    \n",
    "    Args:\n",
    "        results: experiment results from run_sentence_experiments\n",
    "        model_name: name of the model\n",
    "        token_type: type of tokens being tested\n",
    "    \n",
    "    Returns:\n",
    "        dict with statistical analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name.upper()} {token_type.upper()} EXPERIMENT ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    categories = ['target', 'high_interference', 'medium_interference', 'random']\n",
    "    \n",
    "    # Calculate basic success rates\n",
    "    analysis = {}\n",
    "    \n",
    "    for category in categories:\n",
    "        if category not in results or not results[category]:\n",
    "            print(f\"\\nWarning: No results for {category}\")\n",
    "            continue\n",
    "        \n",
    "        # Collect all trial results\n",
    "        count_increases = []\n",
    "        prob_increases = []\n",
    "        total_trials = 0\n",
    "        \n",
    "        for sentence_result in results[category]:\n",
    "            for trial_data in sentence_result['results']['detailed_metrics']:\n",
    "                count_increases.append(1 if trial_data['count_increase'] else 0)\n",
    "                prob_increases.append(1 if trial_data['prob_increase'] else 0)\n",
    "                total_trials += 1\n",
    "        \n",
    "        if total_trials == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calculate success rates\n",
    "        count_success_rate = sum(count_increases) / total_trials\n",
    "        prob_success_rate = sum(prob_increases) / total_trials\n",
    "        \n",
    "        analysis[category] = {\n",
    "            'total_trials': total_trials,\n",
    "            'count_increases': sum(count_increases),\n",
    "            'prob_increases': sum(prob_increases),\n",
    "            'count_success_rate': count_success_rate,\n",
    "            'prob_success_rate': prob_success_rate,\n",
    "            'count_binary_sequence': count_increases,\n",
    "            'prob_binary_sequence': prob_increases\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(f\"  Total trials: {total_trials}\")\n",
    "        print(f\"  Count increases: {sum(count_increases)} ({count_success_rate:.3f})\")\n",
    "        print(f\"  Prob increases: {sum(prob_increases)} ({prob_success_rate:.3f})\")\n",
    "    \n",
    "    # Statistical comparisons with random baseline\n",
    "    if 'random' in analysis:\n",
    "        random_data = analysis['random']\n",
    "        \n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(\"STATISTICAL COMPARISONS vs RANDOM\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        for category in ['target', 'high_interference', 'medium_interference']:\n",
    "            if category not in analysis:\n",
    "                continue\n",
    "            \n",
    "            category_data = analysis[category]\n",
    "            \n",
    "            print(f\"\\n{category.upper()} vs RANDOM:\")\n",
    "            \n",
    "            # Count increase comparison using t-test\n",
    "            count_stat, count_pval = ttest_ind(\n",
    "                category_data['count_binary_sequence'],\n",
    "                random_data['count_binary_sequence'],\n",
    "                equal_var=False  # Welch's t-test\n",
    "            )\n",
    "            \n",
    "            # Probability increase comparison using t-test\n",
    "            prob_stat, prob_pval = ttest_ind(\n",
    "                category_data['prob_binary_sequence'],\n",
    "                random_data['prob_binary_sequence'],\n",
    "                equal_var=False  # Welch's t-test\n",
    "            )\n",
    "            \n",
    "            # Store statistical results\n",
    "            analysis[f'{category}_vs_random'] = {\n",
    "                'count_diff': category_data['count_success_rate'] - random_data['count_success_rate'],\n",
    "                'count_tstat': count_stat,\n",
    "                'count_pvalue': count_pval,\n",
    "                'count_significant': count_pval < 0.05,\n",
    "                'prob_diff': category_data['prob_success_rate'] - random_data['prob_success_rate'],\n",
    "                'prob_tstat': prob_stat,\n",
    "                'prob_pvalue': prob_pval,\n",
    "                'prob_significant': prob_pval < 0.05\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"  Count increase rate difference: {category_data['count_success_rate']:.3f} - {random_data['count_success_rate']:.3f} = {analysis[f'{category}_vs_random']['count_diff']:.3f}\")\n",
    "            print(f\"    T-statistic: {count_stat:.3f}, p-value: {count_pval:.6f}\")\n",
    "            print(f\"    Significant: {'YES' if count_pval < 0.05 else 'NO'}\")\n",
    "            \n",
    "            print(f\"  Prob increase rate difference: {category_data['prob_success_rate']:.3f} - {random_data['prob_success_rate']:.3f} = {analysis[f'{category}_vs_random']['prob_diff']:.3f}\")\n",
    "            print(f\"    T-statistic: {prob_stat:.3f}, p-value: {prob_pval:.6f}\")\n",
    "            print(f\"    Significant: {'YES' if prob_pval < 0.05 else 'NO'}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def print_experiment_summary(analysis: Dict, model_name: str, token_type: str):\n",
    "    \"\"\"Print a concise summary of experiment results\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name.upper()} {token_type.upper()} SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    categories = ['target', 'high_interference', 'medium_interference', 'random']\n",
    "    \n",
    "    print(f\"{'Category':<20} {'Count Rate':<12} {'Prob Rate':<12} {'vs Random':<15}\")\n",
    "    print(f\"{'-'*20} {'-'*12} {'-'*12} {'-'*15}\")\n",
    "    \n",
    "    for category in categories:\n",
    "        if category not in analysis:\n",
    "            continue\n",
    "        \n",
    "        count_rate = analysis[category]['count_success_rate']\n",
    "        prob_rate = analysis[category]['prob_success_rate']\n",
    "        \n",
    "        vs_random_str = \"\"\n",
    "        if f'{category}_vs_random' in analysis:\n",
    "            vs_random = analysis[f'{category}_vs_random']\n",
    "            count_sig = \"**\" if vs_random['count_significant'] else \"\"\n",
    "            prob_sig = \"**\" if vs_random['prob_significant'] else \"\"\n",
    "            vs_random_str = f\"{vs_random['count_diff']:+.3f}{count_sig}/{vs_random['prob_diff']:+.3f}{prob_sig}\"\n",
    "        elif category == 'random':\n",
    "            vs_random_str = \"baseline\"\n",
    "        \n",
    "        print(f\"{category:<20} {count_rate:<12.3f} {prob_rate:<12.3f} {vs_random_str:<15}\")\n",
    "    \n",
    "    print(\"\\n** = statistically significant (p < 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0827d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9: Run single experiment example (template for each token type)\n",
    "# Example for location tokens - modify token_type and model as needed\n",
    "\n",
    "def run_single_token_type_experiment(\n",
    "    model_name: str, \n",
    "    token_type: str, \n",
    "    max_sentences: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    Run experiment for a single model and token type\n",
    "    \n",
    "    Args:\n",
    "        model_name: 'pythia' or 'gpt2'\n",
    "        token_type: 'location', 'person', or 'emotion'\n",
    "        max_sentences: maximum number of sentences to test\n",
    "    \n",
    "    Returns:\n",
    "        tuple of (results, analysis)\n",
    "    \"\"\"\n",
    "    exp_key = f\"{model_name}_{token_type}\"\n",
    "    \n",
    "    if exp_key not in experiment_token_sets:\n",
    "        print(f\"Error: {exp_key} not found in experiment_token_sets\")\n",
    "        return None, None\n",
    "    \n",
    "    exp_data = experiment_token_sets[exp_key]\n",
    "    token_sets = exp_data['token_sets']\n",
    "    target_ids = exp_data['target_ids']\n",
    "    sentences = exp_data['sentences']\n",
    "    \n",
    "    # Get model\n",
    "    if model_name == 'pythia':\n",
    "        model = pythia\n",
    "    elif model_name == 'gpt2':\n",
    "        model = gpt2\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    print(f\"Starting {model_name} {token_type} experiment...\")\n",
    "    print(f\"Target IDs: {len(target_ids)}\")\n",
    "    print(f\"Available sentences: {len(sentences)}\")\n",
    "    \n",
    "    # Run experiments\n",
    "    results = run_sentence_experiments(\n",
    "        model=model,\n",
    "        model_name=model_name,\n",
    "        token_sets=token_sets,\n",
    "        target_ids=target_ids,\n",
    "        sentences=sentences,\n",
    "        max_sentences=max_sentences\n",
    "    )\n",
    "    \n",
    "    # Analyze results\n",
    "    analysis = analyze_experiment_results(\n",
    "        results=results,\n",
    "        model_name=model_name,\n",
    "        token_type=token_type\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print_experiment_summary(analysis, model_name, token_type)\n",
    "    \n",
    "    return results, analysis\n",
    "\n",
    "# Template usage - uncomment and modify as needed:\n",
    "# results, analysis = run_single_token_type_experiment('pythia', 'location', max_sentences=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97923bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_type in token_types:\n",
    "    for model_name in ['pythia', 'gpt2']:\n",
    "        results, analysis = run_single_token_type_experiment(model_name, token_type, max_sentences=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
