{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5708cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "from utils.utils_exp import run_gradient_intervention_on_small_models\n",
    "from utils.utils_model import get_hooked_pythia_70m, get_hooked_gpt2_small\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42df9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"results/gradient_pythia_gpt2\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Serialize the result data\n",
    "def save_experiment_results(results, model_name, suffix=\"\"):\n",
    "    \"\"\"Save experiment results to JSON file\"\"\"\n",
    "    filename = f\"{model_name}_gradient_intervention_results{suffix}.json\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "    \n",
    "    # Convert tensors to serializable format\n",
    "    def convert_to_serializable(obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().numpy().tolist()\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.int32, np.int64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_to_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "    \n",
    "    serializable_results = convert_to_serializable(results)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d952776",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_model = get_hooked_pythia_70m(DEVICE)\n",
    "PYTHIA_CONFIG = {\n",
    "    'model_name': 'pythia',\n",
    "    'layer_types': ['att', 'mlp'],\n",
    "    'layer_range': range(0, 6),\n",
    "    'model': pythia_model,\n",
    "    'tokenizer': None,\n",
    "    'similarity_threshold': 0.2,\n",
    "    'n_target_features': 4,\n",
    "    'n_interference_features': 3,\n",
    "    'n_top_tokens': 3,\n",
    "    'n_test_sentences': 3,\n",
    "    'seed': 50,\n",
    "    'device': DEVICE,\n",
    "    'use_hooked_model': True\n",
    "}\n",
    "\n",
    "# Scale values for intervention\n",
    "scale_values = [0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 17, 20]\n",
    "SCALE_RANGE = [-x for x in scale_values[::-1]] + scale_values\n",
    "\n",
    "print(f\"Pythia Gradient Intervention Configuration:\")\n",
    "print(f\"- Model: {PYTHIA_CONFIG['model_name']}\")\n",
    "print(f\"- Layer types: {PYTHIA_CONFIG['layer_types']}\")\n",
    "print(f\"- Layer range: {list(PYTHIA_CONFIG['layer_range'])}\")\n",
    "print(f\"- Similarity threshold: {PYTHIA_CONFIG['similarity_threshold']}\")\n",
    "print(f\"- Target features: {PYTHIA_CONFIG['n_target_features']}\")\n",
    "print(f\"- Interference features: {PYTHIA_CONFIG['n_interference_features']}\")\n",
    "print(f\"- Top tokens: {PYTHIA_CONFIG['n_top_tokens']}\")\n",
    "print(f\"- Test sentences: {PYTHIA_CONFIG['n_test_sentences']}\")\n",
    "print(f\"- Scale range: {len(SCALE_RANGE)} values from {min(SCALE_RANGE)} to {max(SCALE_RANGE)}\")\n",
    "print(f\"- Using HookedTransformer: {PYTHIA_CONFIG['use_hooked_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f993345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Pythia gradient intervention experiment\n",
    "print(\"Starting Pythia gradient intervention experiment...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    pythia_results = run_gradient_intervention_on_small_models(\n",
    "        scale_range=SCALE_RANGE,\n",
    "        **PYTHIA_CONFIG\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nPythia gradient intervention experiment completed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error running Pythia gradient intervention experiment: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    pythia_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pythia_results:\n",
    "    import time\n",
    "    timestamp = str(int(time.time()))\n",
    "    thres = PYTHIA_CONFIG['similarity_threshold']\n",
    "    pythia_filepath = save_experiment_results(pythia_results, 'pythia', f'_{thres}_{timestamp}')\n",
    "\n",
    "    # Print detailed statistics\n",
    "    total_features = 0\n",
    "    total_tests = 0\n",
    "    layer_stats = {}\n",
    "\n",
    "    for layer_type in pythia_results:\n",
    "        layer_stats[layer_type] = {}\n",
    "        for layer_idx in pythia_results[layer_type]:\n",
    "            features_in_layer = len(pythia_results[layer_type][layer_idx])\n",
    "            layer_stats[layer_type][layer_idx] = features_in_layer\n",
    "            total_features += features_in_layer\n",
    "            \n",
    "            # Count total tests (tokens x sentences)\n",
    "            for feature_id in pythia_results[layer_type][layer_idx]:\n",
    "                feature_data = pythia_results[layer_type][layer_idx][feature_id]\n",
    "                total_tests += len(feature_data.get('tests', {}))\n",
    "\n",
    "    print(f\"\\nPythia Gradient Intervention Results Summary:\")\n",
    "    print(f\"- Total features tested: {total_features}\")\n",
    "    print(f\"- Total token tests: {total_tests}\")\n",
    "    print(f\"- Layer types: {list(pythia_results.keys())}\")\n",
    "\n",
    "    for layer_type, layers in layer_stats.items():\n",
    "        print(f\"\\n{layer_type.upper()} layers:\")\n",
    "        for layer_idx, feature_count in layers.items():\n",
    "            print(f\"  Layer {layer_idx}: {feature_count} features\")\n",
    "            \n",
    "    print(f\"\\nFull results saved to: {pythia_filepath}\")\n",
    "else:\n",
    "    print(\"No Pythia results to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09589542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 gradient intervention experiment\n",
    "import time\n",
    "\n",
    "# Load GPT-2 HookedTransformer model\n",
    "gpt2_model = get_hooked_gpt2_small(DEVICE)\n",
    "\n",
    "GPT2_CONFIG = {\n",
    "    'model_name': 'gpt2',\n",
    "    'layer_types': ['att', 'mlp'],\n",
    "    'layer_range': range(0, 12),\n",
    "    'model': gpt2_model,\n",
    "    'tokenizer': None,\n",
    "    'n_target_features': 4,\n",
    "    'n_interference_features': 3,\n",
    "    'n_top_tokens': 5,\n",
    "    'n_test_sentences': 3,\n",
    "    'seed': 53,\n",
    "    'device': DEVICE,\n",
    "    'use_hooked_model': True\n",
    "}\n",
    "\n",
    "semantic_thresholds = [0.4, 0.3, 0.2, 0.15]\n",
    "\n",
    "scale_values = [0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 17, 20]\n",
    "SCALE_RANGE = [-x for x in scale_values[::-1]] + scale_values\n",
    "\n",
    "print(f\"Starting GPT-2 gradient intervention experiments with semantic thresholds: {semantic_thresholds}\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Layer types: {GPT2_CONFIG['layer_types']}\")\n",
    "print(f\"- Layer range: {list(GPT2_CONFIG['layer_range'])}\")\n",
    "print(f\"- Target features: {GPT2_CONFIG['n_target_features']}\")\n",
    "print(f\"- Interference features: {GPT2_CONFIG['n_interference_features']}\")\n",
    "print(f\"- Top tokens: {GPT2_CONFIG['n_top_tokens']}\")\n",
    "print(f\"- Test sentences: {GPT2_CONFIG['n_test_sentences']}\")\n",
    "print(f\"- Scale range: {len(SCALE_RANGE)} values from {min(SCALE_RANGE)} to {max(SCALE_RANGE)}\")\n",
    "print(f\"- Using HookedTransformer: {GPT2_CONFIG['use_hooked_model']}\")\n",
    "\n",
    "# run experiments for each threshold\n",
    "gpt2_results = {}\n",
    "total_start_time = time.time()\n",
    "\n",
    "for i, threshold in enumerate(semantic_thresholds):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Running GPT-2 gradient intervention experiment {i+1}/{len(semantic_thresholds)} with similarity threshold: {threshold}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    config = GPT2_CONFIG.copy()\n",
    "    config['similarity_threshold'] = threshold\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = run_gradient_intervention_on_small_models(\n",
    "            scale_range=SCALE_RANGE,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        gpt2_results[threshold] = results\n",
    "        \n",
    "        total_features = 0\n",
    "        total_tests = 0\n",
    "        layer_stats = {}\n",
    "        \n",
    "        for layer_type in results:\n",
    "            layer_stats[layer_type] = {}\n",
    "            for layer_idx in results[layer_type]:\n",
    "                features_in_layer = len(results[layer_type][layer_idx])\n",
    "                layer_stats[layer_type][layer_idx] = features_in_layer\n",
    "                total_features += features_in_layer\n",
    "                \n",
    "                for feature_id in results[layer_type][layer_idx]:\n",
    "                    feature_data = results[layer_type][layer_idx][feature_id]\n",
    "                    total_tests += len(feature_data.get('tests', {}))\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        total_elapsed = time.time() - total_start_time\n",
    "        \n",
    "        print(f\"\\nCompleted threshold {threshold} in {elapsed_time:.2f} seconds\")\n",
    "        print(f\"Total elapsed time: {total_elapsed:.2f} seconds\")\n",
    "        print(f\"Statistics for threshold {threshold}:\")\n",
    "        print(f\"- Total features tested: {total_features}\")\n",
    "        print(f\"- Total token tests: {total_tests}\")\n",
    "        print(f\"- Layer types: {list(results.keys())}\")\n",
    "        \n",
    "        for layer_type, layers in layer_stats.items():\n",
    "            layer_feature_count = sum(layers.values())\n",
    "            print(f\"  {layer_type.upper()}: {layer_feature_count} features across {len(layers)} layers\")\n",
    "        \n",
    "        timestamp = str(int(time.time()))\n",
    "        filepath = save_experiment_results(\n",
    "            gpt2_results[threshold], \n",
    "            'gpt2', \n",
    "            f'_{threshold}_{timestamp}'\n",
    "        )\n",
    "        \n",
    "        print(f\"- Results saved to: {filepath}\")\n",
    "        \n",
    "        remaining_thresholds = len(semantic_thresholds) - (i + 1)\n",
    "        if remaining_thresholds > 0:\n",
    "            avg_time_per_threshold = total_elapsed / (i + 1)\n",
    "            estimated_remaining = avg_time_per_threshold * remaining_thresholds\n",
    "            print(f\"- Estimated remaining time: {estimated_remaining:.1f} seconds ({estimated_remaining/60:.1f} minutes)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error running experiment with threshold {threshold}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        gpt2_results[threshold] = {'error': str(e)}\n",
    "        continue\n",
    "\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"All GPT-2 gradient intervention experiments completed!\")\n",
    "print(f\"Total execution time: {total_elapsed_time:.2f} seconds ({total_elapsed_time/60:.1f} minutes)\")\n",
    "print(f\"Successfully completed thresholds: {[t for t in gpt2_results.keys() if 'error' not in gpt2_results[t]]}\")\n",
    "print(f\"Failed thresholds: {[t for t in gpt2_results.keys() if 'error' in gpt2_results[t]]}\")\n",
    "\n",
    "print(f\"\\nOverall GPT-2 Gradient Intervention Results Summary:\")\n",
    "for threshold in semantic_thresholds:\n",
    "    if threshold in gpt2_results and 'error' not in gpt2_results[threshold]:\n",
    "        results = gpt2_results[threshold]\n",
    "        total_features = sum(len(results[lt][li]) for lt in results for li in results[lt])\n",
    "        total_tests = sum(len(results[lt][li][fid].get('tests', {})) \n",
    "                         for lt in results for li in results[lt] for fid in results[lt][li])\n",
    "        \n",
    "        print(f\"Threshold {threshold}:\")\n",
    "        print(f\"  - Features: {total_features}\")\n",
    "        print(f\"  - Tests: {total_tests}\")\n",
    "        print(f\"  - Layer types: {list(results.keys())}\")\n",
    "    else:\n",
    "        print(f\"Threshold {threshold}: FAILED\")\n",
    "\n",
    "print(f\"\\nAll result files are saved in the '{results_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbe718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "\n",
    "def plot_gradient_intervention_results(data_path, output_dir=\"analysis_results\", model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    A temporary plot function for visualization.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if '*' in data_path:\n",
    "        files = glob.glob(data_path)\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No files found matching pattern: {data_path}\")\n",
    "        data_path = sorted(files)[-1]\n",
    "        print(f\"Using latest file: {data_path}\")\n",
    "    \n",
    "    with open(data_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded results from: {data_path}\")\n",
    "    \n",
    "    metrics_data = {\n",
    "        'weighted_cosine_similarity': defaultdict(list),\n",
    "        'spearman_correlation': defaultdict(list),\n",
    "        'kendall_correlation': defaultdict(list),\n",
    "        'weighted_overlap': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    metric_names = {\n",
    "        'weighted_cosine_similarity': 'Weighted Cosine Similarity',\n",
    "        'spearman_correlation': 'Spearman Correlation',\n",
    "        'kendall_correlation': 'Kendall Correlation',\n",
    "        'weighted_overlap': 'Weighted Overlap'\n",
    "    }\n",
    "    \n",
    "    print(\"Processing gradient intervention results...\")\n",
    "    \n",
    "    total_processed = 0\n",
    "    total_improved = 0\n",
    "    improvement_counts = {metric: defaultdict(int) for metric in metrics_data.keys()}\n",
    "    \n",
    "    for layer_type in results:\n",
    "        for layer_index in results[layer_type]:\n",
    "            for feature_id in results[layer_type][layer_index]:\n",
    "                feature_data = results[layer_type][layer_index][feature_id]\n",
    "                \n",
    "                for token_idx in feature_data.get('tests', {}):\n",
    "                    token_data = feature_data['tests'][token_idx]\n",
    "                    \n",
    "                    for sent_idx in token_data.get('sentences', {}):\n",
    "                        sent_data = token_data['sentences'][sent_idx]\n",
    "                        \n",
    "                        if 'baseline' not in sent_data:\n",
    "                            continue\n",
    "                        \n",
    "                        baseline = sent_data['baseline']\n",
    "                        \n",
    "                        for level in sent_data:\n",
    "                            if level == 'baseline':\n",
    "                                continue\n",
    "                            \n",
    "                            level_data = sent_data[level]\n",
    "                            \n",
    "                            for metric in metrics_data.keys():\n",
    "                                baseline_val = baseline.get(metric, 0)\n",
    "                                best_val = level_data.get(f'best_{metric}', 0)\n",
    "                                \n",
    "                                is_improved = False\n",
    "                                percentage_improvement = 0\n",
    "                                \n",
    "                                if metric in ['weighted_cosine_similarity', 'weighted_overlap']:\n",
    "                                    is_improved = best_val > baseline_val\n",
    "                                    if is_improved and abs(baseline_val) > 1e-8:\n",
    "                                        percentage_improvement = (best_val - baseline_val) / abs(baseline_val) * 100\n",
    "                                        \n",
    "                                elif metric in ['spearman_correlation', 'kendall_correlation']:\n",
    "                                    is_improved = best_val > baseline_val\n",
    "                                    if is_improved and abs(baseline_val) > 1e-8:\n",
    "                                        percentage_improvement = (best_val - baseline_val) / abs(baseline_val) * 100\n",
    "                                \n",
    "                                if is_improved and percentage_improvement > 0:\n",
    "                                    metrics_data[metric][level].append(percentage_improvement)\n",
    "                                    improvement_counts[metric][level] += 1\n",
    "                                    total_improved += 1\n",
    "                        \n",
    "                        total_processed += 1\n",
    "    \n",
    "    print(f\"Processed {total_processed} test cases\")\n",
    "    print(f\"Total improvements recorded: {total_improved}\")\n",
    "\n",
    "    all_levels = set()\n",
    "    for metric in metrics_data:\n",
    "        all_levels.update(metrics_data[metric].keys())\n",
    "    \n",
    "    level_order = ['self']\n",
    "    \n",
    "    range_levels = [l for l in all_levels if '-' in l and l != 'self']\n",
    "    range_levels.sort(key=lambda x: float(x.split('-')[0]) if x.split('-')[0].replace('.','').isdigit() else 0, reverse=True)\n",
    "    level_order.extend(range_levels)\n",
    "    \n",
    "    if 'rand' in all_levels:\n",
    "        level_order.append('rand')\n",
    "    \n",
    "    other_levels = [l for l in all_levels if l not in level_order]\n",
    "    level_order.extend(sorted(other_levels))\n",
    "    \n",
    "    print(f\"Gradient interference levels found: {level_order}\")\n",
    "    \n",
    "    level_labels = {}\n",
    "    for level in level_order:\n",
    "        if level == 'self':\n",
    "            level_labels[level] = 'Original'\n",
    "        elif level == 'rand':\n",
    "            level_labels[level] = 'Random'\n",
    "        elif '-' in level and level.count('-') == 1:\n",
    "            try:\n",
    "                parts = level.split('-')\n",
    "                min_val = float(parts[0])\n",
    "                if min_val >= 0.4:\n",
    "                    level_labels[level] = \"High\"\n",
    "                elif min_val >= 0.3:\n",
    "                    level_labels[level] = \"Mid High\"\n",
    "                elif min_val >= 0.2:\n",
    "                    level_labels[level] = \"Mid\"\n",
    "                elif min_val >= 0.1:\n",
    "                    level_labels[level] = \"Mid Low\"\n",
    "                else:\n",
    "                    level_labels[level] = \"Low\"\n",
    "            except:\n",
    "                level_labels[level] = level.title()\n",
    "        else:\n",
    "            level_labels[level] = level.title()\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 4, figsize=(17, 4.5), dpi=300)\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    colors = ['#B71C1C', '#C62828', '#D32F2F', '#E53935', '#F44336', '#EF5350', '#E57373']\n",
    "    \n",
    "    metrics = ['weighted_cosine_similarity', 'spearman_correlation', 'kendall_correlation', 'weighted_overlap']\n",
    "    titles = ['Weighted Cosine Similarity', 'Spearman Correlation', 'Kendall Correlation', 'Weighted Overlap']\n",
    "    y_labels = [\n",
    "        'Mean % Improvement',\n",
    "        'Mean % Improvement', \n",
    "        'Mean % Improvement',\n",
    "        'Mean % Improvement'\n",
    "    ]\n",
    "    \n",
    "    for i, (metric, title, y_label) in enumerate(zip(metrics, titles, y_labels)):\n",
    "        ax = axs[i]\n",
    "\n",
    "        means = []\n",
    "        sems = []\n",
    "        labels = []\n",
    "        counts = []\n",
    "        \n",
    "        for level in level_order:\n",
    "            if level in metrics_data[metric] and len(metrics_data[metric][level]) > 0:\n",
    "                values = metrics_data[metric][level]\n",
    "                means.append(np.mean(values))\n",
    "                sems.append(np.std(values) / np.sqrt(len(values)))\n",
    "                labels.append(level_labels[level])\n",
    "                counts.append(len(values))\n",
    "            else:\n",
    "                means.append(0)\n",
    "                sems.append(0)\n",
    "                labels.append(level_labels.get(level, level))\n",
    "                counts.append(0)\n",
    "        \n",
    "        x_positions = np.arange(len(means)) + 1\n",
    "        \n",
    "        bars = ax.bar(x_positions, means, width=0.6, color=colors[:len(means)], \n",
    "                      yerr=sems, capsize=5, \n",
    "                      error_kw={'ecolor': 'black', 'linewidth': 1, 'capthick': 1})\n",
    "        \n",
    "        for idx, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            if height > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height + sems[idx] + max(means)*0.01,\n",
    "                       f'{height:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax.set_title(title, fontsize=12, pad=10)\n",
    "        ax.set_xlabel('Gradient Interference Level', fontsize=10)\n",
    "        ax.set_ylabel(y_label, fontsize=10)\n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(labels, rotation=45)\n",
    "        ax.grid(axis='y', linestyle='-', alpha=0.2)\n",
    "        \n",
    "        ax.set_ylim(bottom=0)\n",
    "        \n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} Gradient Intervention', fontsize=14, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    model_lower = model_name.lower().replace(' ', '_')\n",
    "    png_path = os.path.join(output_dir, f\"{model_lower}_gradient_intervention_results.png\")\n",
    "    pdf_path = os.path.join(output_dir, f\"{model_lower}_gradient_intervention_results.pdf\")\n",
    "    \n",
    "    plt.savefig(png_path, bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(pdf_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nResults saved:\")\n",
    "    print(f\"- Plot (PNG): {png_path}\")\n",
    "    print(f\"- Plot (PDF): {pdf_path}\")\n",
    "    \n",
    "    return metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0737764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "if True:\n",
    "    print(\"Plotting GPT2-Small gradient intervention results...\")\n",
    "    plot_gradient_intervention_results(\n",
    "        data_path=\"results/gradient_pythia_gpt2/gpt2_gradient_intervention_results_0.15_(YOUR_TIMESTAMP).json\",\n",
    "        output_dir=\"analysis_results\",\n",
    "        model_name=\"GPT2-Small\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No Pythia results available for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27166b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp-arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
